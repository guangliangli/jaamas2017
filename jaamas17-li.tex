%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
greeter
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage{array}
\usepackage{textcomp}
\usepackage{float}
\usepackage{url}
\usepackage{amsmath}
\usepackage{epstopdf}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[usenames,dvipsnames]{color}
\usepackage{rotating}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{caption}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}

% \newcommand{\HH}[1]{\textcolor{Red}{#1}}
\long\def\HH#1{\textcolor{green}{**[Hayley] #1**}}
\long\def\SW#1{\textcolor{red}{**[Shimon] #1**}}
\long\def\GL#1{\textcolor{red}{**[Guangliang] #1**}}


\newcommand{\edit}[1]{\textcolor{ProcessBlue}{#1}}

%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%


\begin{document}

\title{\textcolor{blue}{Effect of agent's social competitive feedback and human's facial expressions on reinforcement agent learning in the TAMER framework}%An investigation on training user interface with social competitive feedback and facial expressions for reinforcement agent learning in the TAMER framework}
%Effects of social competitive feedback and facial expression for agent's learning in the TAMER framework}
%An investigation on facial expressions as human rewards for agent's learning in the TAMER framework}%An investigation of facial expressions as implicit evaluative feedback for agent learning from human reward%Reinforcement Learning from Facial Expressions as Implicit Human Reward %Building 
%Gamifying human-agent interaction for effective agent's learning from human reward
%Social interaction for effective agent's learning from human reward
%Understanding Human-Agent Interaction to Build Efficient Learning Agents
%Learning from Human Reward Benefits from Socio-Competitive Feedback%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{Investigation of agent learning from facial expressions as evaluative feedback}        % if too long for running head

\author{Guangliang Li  \and Hamdi Dibeklio{\u{g}}lu  \and Shimon Whiteson \and 
        Hayley Hung %\HH{for strange rules related to my tenure, I need to be last author on papers where I supervise a phd student... can you make sure that for our future publications, you always put me as last author? I already discussed with Shimon and he was fine with this.}%etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Guangliang Li \at
             %Informatics Institute \\
             Ocean University of China \\
              \email{guangliangli@ouc.edu.cn}  
           \and 
           Hamdi Dibeklio{\u{g}}lu \at
           Delft University of Technology \\
           \email{h.dibeklioglu@tudelft.nl}
           \and   
            Shimon Whiteson \at 
            University of Oxford \\
            \email{shimon.whiteson@cs.ox.ac.uk}          
           \and
           Hayley Hung \at
           Delft University of Technology \\
           \email{h.hung@tudelft.nl}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

The \emph{TAMER} framework provides a way for agents to learn to solve tasks using human-generated rewards.
Previous research showed that humans give copious feedback early in training but very sparsely thereafter and that an agent's competitive feedback --- informing the trainer about its performance relative to other trainers --- can greatly affect the trainer's engagement and the agent's learning. 
%In this paper, we present the results of the first large-scale study of TAMER, which was performed at a major science museum and involved 561 subjects. 
In this article, we present the first large-scale study of TAMER, involving 561 participants, which investigates the effect of the agent's competitive feedback in a new setting as well as the potential of agent learning from trainers' facial expressions via interpreting them as \textcolor{blue}{%implicit 
evaluative feedback}. Our results show for the first time that a TAMER agent is tested and shown to successfully learn to play Infinite Mario, a challenging reinforcement-learning benchmark problem. %based on the popular video game, given feedback from both adult ($N=209$) and child ($N=352$) trainers.
In addition, our study supports prior results demonstrating the importance of bi-directional feedback and competitive elements in the training interface. Finally, our results shed light on the potential for using trainers' facial expressions as reward signals. %as well as the role of age and gender in trainer behavior and agent performance.


%Insert your abstract here. Include keywords, PACS and mathematical
%subject classification numbers as needed.
\keywords{\textcolor{blue}{reinforcement learning \and facial expressions \and human agent interaction \and interactive reinforcement learning} }%learning from human reward \and implicit evaluative feedback}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{sec:introduction}
%Your text comes here. Separate text sections with

Socially intelligent autonomous agents have the potential to become our high-tech companions in the family of the future. The ability of these intelligent agents to efficiently learn from non-technical users to perform a task in a natural way will be key to their success. Therefore, it is critical to develop methods that facilitate the interaction between these non-technical users and agents, through which they can transfer task knowledge effectively to such agents.

\textcolor{blue}{Interactive reinforcement learning %from human reward, i.e., evaluations of the quality of the agent's behavior, 
has proven to be a powerful technique for facilitating the teaching of artificial agents by their human users \cite{isbell2001social,thomaz2008teachable,knox2009interactively}. In interactive reinforcement learning, an agent learns from human reward, i.e., evaluations of the quality of the agent's behavior provided by a human user, in a reinforcement learning framework.
Compared to learning from demonstration \cite{argall2009survey}, %learning from human reward 
interactive reinforcement learning does not require the human to be able to perform the task well herself; she needs only to be a good judge of performance.  Nonetheless, agent learning from human reward is limited by the quality of the interaction between the human trainer and agent.} %quality and quantity of feedback provided by the human.  
%Intuitively, over time humans get tired of giving the \emph{explicit feedback}, e.g., button presses to indicate positive or negative reward, on which TAMER relies.  In fact, several TAMER studies have shown that humans give copious feedback early in training but only very sparsely thereafter \cite{li2013using,knox2012humans}. 

%Intuitively, 
\textcolor{blue}{Previous research shows that in interactive reinforcement learning, the interaction between the agent and trainer should ideally be bi-directional \cite{li2013using}. %: not only should the trainer give the agent the feedback it needs for learning, the agent should explicitly give the trainer feedback on how well that learning is going. 
From the agent's point of view, the way that the agent interacts with the human trainer can greatly affect the trainer's engagement and the agent's learning. In particular, Li et al.\ \cite{li2016using,li2017social} showed that if an agent informs the trainer socio-competitive feedback --- information about the agent's past and current performance and its performance relative to others, the trainer will provide more feedback and the agent will ultimately perform better. }
%\HH{I dont' understand this sentence in and why the citations have been placed where they have. Can you clarify this part?}

%However, due to the difficulty of recruiting subjects, these studies of Li et al., like others \cite{knox2012humans} evaluating TAMER---a popular method for facilitating autonomous agents to learn from human reward \cite{knox2009interactively}, were conducted using only 50-100 subjects. 

%Moreover, from the human's point of view, over time humans get tired of giving the \emph{explicit feedback}, e.g., button presses to indicate positive or negative reward, on which TAMER relies.  In fact, several TAMER studies have shown that humans give copious feedback early in training but only very sparsely thereafter \cite{li2013using,knox2012humans}.

 %due to the difficulty of recruiting subjects, these studies, like others \cite{knox2012humans} %\HH{Can you add one or two citations from Brad's work that uses these small numbers of subjects?}
%evaluating TAMER---a popular method for facilitating autonomous agents to learn from human reward, were conducted using only 50-100 subjects. %Moreover, over time humans get tired of giving the \emph{explicit feedback}, e.g., button presses to indicate positive or negative reward, on which TAMER relies.  In fact, several TAMER studies have shown that humans give copious feedback early in training but only very sparsely thereafter \cite{li2013using,knox2012humans}. 

In this article, we present the results of the first large-scale study of TAMER --- \textcolor{blue}{a popular interactive reinforcement learning method for enabling autonomous agents to learn from human reward \cite{knox2009interactively}} --- by implementing it in the Infinite Mario domain. 
Our study, involving 561 participants, was conducted at the NEMO science museum in Amsterdam using museum visitors (aged 6 to 72). We investigate the effect of the agent's socio-competitive feedback in a new setting 
where people who know each other train at the same time in the same room and receive competitive feedback about their performance relative to each other. %and hypothesize that `competition' will result in better performed agents.

In addition, from the human's point of view, as time progresses, humans may get tired of giving \emph{explicit feedback} (e.g., button presses to indicate positive or negative reward) on which TAMER relies. In fact, several TAMER studies have shown that humans give copious feedback early in training but very sparsely thereafter \cite{knox2012humans,li2013using}. Therefore, in our study, we investigate the potential of using facial expressions as reward signals, which have been often used by humans to consciously or subconsciously encourage or discourage specific behaviors they want to teach, e.g., smiling to indicate good behavior and frowning to indicate bad behavior \cite{vail1994emotion}, instead of button presses.
%Therefore, in our study, we also investigate the potential of enabling agents to learn from human trainer's facial expressions. 
To examine this potential, we recorded the facial expressions of all trainers during training and, in some conditions, told participants that their facial expressions 
%\HH{Guangliang, I'm not sure if this was true but I think it needs to be said as the facial expression condition was actually also purposefully encouraging explicit feedback: (happy and sad expressions would map to positive and negative reward respectively)} 
would be used as encouraging explicit feedback, e.g., happy and sad expressions would map to positive and negative reward respectively, 
%would be used 
in addition to key presses, to train the agent. However, due to the significant challenge of processing facial expressions sufficiently accurately online and in real time in a fairly unconstrained non-laboratory setting, only key presses were actually used for agent learning. 

In our experiment, we test two independent variables: `competition'---whether the agent will inform competitive feedback to the trainer, and `facial expression'\---whether trainers were told that their facial expressions would be used in addition to key presses to train the agent. The main idea of the facial expression condition is to examine the effect that the additional modality of facial expressions could have on the agent's learning and prediction of positive or negative evaluative feedback based on facial expressions. %cognitive load of trainers and whether this varies depending on age or gender.

We investigate how `competition' and `facial expression' affect %the amount of feedback received by the agent, 
the agent's learning performance and trainer's facial expressiveness in four experimental conditions in our study: the \emph{control condition}---without `competition' or `facial expression', the \emph{facial expression condition}---without `competition' but with `facial expression', the \emph{competitive condition}---with `competition' but without `facial expression', and the \emph{competitive facial expression con\-dition}---with both. \textcolor{blue}{We hypothesize that `competition' will result in better performing agents, and `facial expression' will %reduce the number of keypress feedback received from the trainer and 
result in worse agent performance.} %\textcolor{blue}{In addition, we expect that %both `competition' and 
%`facial expression' will increase the trainer's facial expressiveness since trainers may make posed expressions.}

\textcolor{blue}{Our experimental results show for the first time that a TAMER agent is %shown to be 
able to successfully learn to play Infinite Mario, a challenging reinforcement learning benchmark problem ba\-sed on the popular video game.} %Moreover, we had substantial representation of both children and adults and across both genders, which enabled more extensive age-dependent and gender-dependent analyses. 
Moreover, our study provides a large-scale support of the results of Li et al.\ \cite{li2016using,li2017social} demonstrating the importance of bi-directional feedback and competitive elements in the training interface. %and indicates negative effects of `facial expression' on agent's learning. 
%In addition, %while examining the gender and age, 
%our results show that telling female trainers to use facial expressions has a significant negative effect on the agent's learning, %when they are trained by female subjects, 
%especially those who are less than 13 years old and cannot train agents to perform well. %train agents insufficiently well. 
%That is, telling trainers to use facial expressions lowered the quality of the key press feedback given by them. %although the amount of feedback was not affected. %In addition, %overall, the agent's competitive feedback can significantly improve an agent's learning especially trained by %male subjects, 
%This suggests that while the use of facial expressions for agent training seems sensible, there are number of factors to consider relating to additional cognitive load of making posed facial expressions which leads to a reduced quality in key-press feedback. %but now in a new setting in which a small group of subjects train at the same time in the same room and receive competitive feedback about their performance relative to each other. 
Furthermore, our analysis shows that telling trainers to use facial expressions makes them inclined to exaggerate their expressions, resulting in higher accuracies for estimating their corresponding positive and negative feedback keypresses using facial expressions. Moreover, competition can elevate facial expressiveness and further increase the predicted accuracy. \textcolor{blue}{Finally, our results show the possibility of an agent learning from facial expressions via interpreting them as %implicit 
evaluative feedback. %though still worse than from explicit feedback. 
To our knowledge, it is the first time facial expressions %, or any kind of implicit feedback 
has been shown to work in TAMER, opening the door to a much greater potential for learning from human reward in more natural, personalized and possibly more long term learning scenarios.}

The rest of this article starts with a review of the related work in Section \ref{sec:rw}. \textcolor{blue}{Section \ref{sec:irl} presents an introduction on interactive reinforcement learning. In Section \ref{sec:tamer} we provide the background and details about TAMER framework and Section \ref{sec:domain} describes the Infinite Mario domain we used in our user study and the implemented representation of it for TAMER agent learning}. Section \ref{sec:es} describes the experimental setup and Section \ref{sec:cons} describes the proposed experimental conditions. Section \ref{sec:re} reports and discusses the experimental results. Section \ref{sec:doq} discusses the open questions for learning from facial expressions. Finally, Section \ref{sec:con} concludes. 

%\vspace{-3mm}
\section{Related work}
\label{sec:rw}

\textcolor{blue}{Our work contributes to a growing literature on interactive reinforcement learning, which deals with how an agent should learn the behavior from reward provided by a live human trainer rather than from the usual pre-coded reward function in a reinforcement learning framework~\cite{isbell2001social,knox2009interactively,tenorio2010dynamic,pilarski2011online,suay2011effect}. Reward provided by a live human trainer is termed ``'human reward' and reward from the usual pre-coded reward function is termed  ``environmental reward'' in reinforcement learning. %If reward in a reinforcement learning framework is given by a live human trainer as he or she observes the agent?s behavior?rather than from the usual pre-coded reward function?how should an agent use these feedback signals to best learn the behavior that the human intends to teach 
%learning from human reward, in which 
In interactive reinforcement learning, a human trainer evaluates the quality of an agent's behavior and gives the agent feedback to improve its behavior. This kind of feedback can be restricted to express various intensities of approval and disapproval and mapped to numeric ``reward'' for the agent to revise its behavior. %In contrast to learning from demonstration \cite{argall2009survey}, learning from human reward requires only a simple task-independent interface, potentially less expertise and cognitive load from the trainer~\cite{knoxreinforcement}. 
We will provide details about interactive reinforcement learning in Section \ref{sec:bg}. 
In this section, we will review literatures on reinforcement learning from human reward, and machine learning systems or agents learning from facial expressions.}

\subsection{\textcolor{blue}{Reinforcement Learning from Human Reward}}

\emph{Clicker training} \cite{blumberg2002integrated} is a related concept that involves using only positive reward to train an agent. %It is a form of animal training in which the sound of an audible device such as a clicker or whistle is associated with a primary reinforcer such as food and then used as a reward signal to guide the agent towards desired behavior. 
Isbell et al.\ \cite{isbell2001social} developed the first software agent called Cobot that learns from both reward and punishment
%In the first work using both reward and punishment to train an artificial agent, Isbell et al.\ \shortcite{isbell2001social} develop a software agent called Cobot %is \HH{developed or trained? 'developed' implies that the the agent was taught how to write it's own software. I think you need to be more specific about what the task was tha required proactive actions} developed 
by applying reinforcement learning in an online text-based virtual world where people interact. The agent learns to take proactive verbal actions (e.g. proposing a topic for conversation) from `reward and punish' text-verbs invoked by multiple users. %users interact with each other. The agent learned to take proactive actions from multiple sources of human reward, which are `reward and punish' text-verbs invoked by multiple users.
\textcolor{blue}{Later, Thomaz and Breazeal \cite{thomaz2008teachable} implemented an interface with a tabular \emph{Q-learning} \cite{watkins1992q} agent. In their interface, a separate interaction channel is provided to allow the human to give the agent feedback.} The agent aims %\HH{since the related work section is in the past tense, make sure that the tense is consistent} 
to maximize its total discounted sum of human reward and environmental reward. They treat the human's feedback as additional reward that supplements the environmental reward. \textcolor{blue}{Their results show an improvement in agent performance by allowing the trainer to give action advice on top of human reward.} 
Suay and Chernova \cite{suay2011effect} extend their work to a real-world robotic system using only human reward.% without environmental reward. %\HH{I think you need to say what type of feedback was used to distinguish from our work.}

Knox and Stone \cite{knox2009interactively} propose the \emph{TAMER} framework that allows an agent to learn from only human reward signals instead of environmental rewards 
by directly modeling it. %the human reward. 
With TAMER as a tool, Knox et al.\ \cite{knox2012humans} study how humans teach agents by examining their responses to changes in their perception of the agent and changes in 
the agent's behavior. \textcolor{blue}{They deliberately reduce the quality of the agent's behavior whenever the rate of human feedback decreases, and found that the agent can elicit more feedback from the human trainer but with lower agent performance.} %They found that the agent can elicit more feedback from the human trainer but with lower performance if the quality of the agent's behavior is deliberately reduced whenever the rate of human feedback decreases.} 
\textcolor{blue}{In addition, Li et al.\ \cite{li2013using,li2014learning} investigate how the trainer's behavior in TAMER is affected when an agent gives the trainer feedback. For example, they allow the agent to display informative feedback about its past and present performance, and competitive feedback about the agent's performance relative to other trainers.} However, in this article, we investigate the effect of agent's competitive feedback on the trainer's training behavior in a different setting where a small group of closely related subjects train at the same time in the same room. %especially when she is told that she can use both key presses and facial expressions to train the agent. %Specifically, they allowed the agent to display informative feedback about the agent's past and present performance, as well as socio-competitive feedback about the agent's performance relative to other trainers. Their results showed that the agent's performance was improved when it provides such feedback to the human trainer.  Similar to the work of Li et al., in this paper, we also propose a competitive condition to investigate how the agent's competitive feedback affects the trainer's training behavior in different genders and ages, especially when she is told that she can use both key presses and facial expressions to train the agent. 

\textcolor{blue}{Similar to the TAMER framework, %especially VI-TAMER \cite{knox2015framing}, 
Pilarski et al.\ \cite{pilarski2011online} proposed a continuous action actor-critic reinforcement learning algorithm \cite{grondman2012survey} that learns an optimal control policy for a simulated upper-arm robotic prosthesis using only human-delivered reward signals. Their algorithm does not model the human reward signals %but treats them the same as the environmental rewards in traditional RL. Therefore, the agent 
and tries to learn a policy to receive the most discounted accumulated human reward.%Their algorithm does not model the human reward signals but treats them the same as the environmental rewards in traditional RL. Therefore, the agent tries to learn a policy to receive the most discounted accumulated human reward.
}

Recently, MacGlashan et al. \cite{macglashanconvergent} propose an Actor-Critic algorithm to incorporate human-delivered reinforcement. Specifically, they assume that the human trainer employs a diminishing returns strategy, which means the initial human feedback for taking the optimal action $a$ in state $s$ will be positive, but goes to zero as the probability of selecting action $a$ in state $s$ goes to 1. \textcolor{blue}{Based on this assumption, they take the human reward as an Advantage Function (Temporal Difference in traditional Reinforcement Learning is an unbiased estimate of advantage function), which describes how much better or worse an action selection is compared to the current expected behavior. Then they use human reward to directly %myopically 
update the policy function.}

While the work mentioned above interprets human feedback as a numeric reward, Loftin et al.\ \cite{loftin2015learning} interpret human feedback as categorical feedback strategies that depend both on the behavior the trainer is trying to teach and the trainer's teaching strategy. \textcolor{blue}{They infer knowledge about the desired behavior from cases where no feedback is provided. They show that their algorithms could learn faster than algorithms that treat the feedback as a numeric reward.} In addition, Griffith et al.\ \cite{griffith2013policy} propose an approach called `policy shaping' by formalizing the meaning of human feedback as a label on the optimality of actions and using it directly as policy advice, instead of converting feedback signals into evaluative rewards. 

Therefore, there are several possibilities to take facial expressions as evaluative feedback for an autonomous agent to learn to perform a task, e.g., numeric reward as in the work of Thomaz and Breazeal \cite{thomaz2008teachable} and TAMER \cite{knox2009interactively}, or action feedback as in SABL \cite{loftin2015learning} and policy advice as in policy shaping \cite{griffith2013policy}. %and how to facilitate an agent to learn from it remain to be further explored in future work. 
\textcolor{blue}{In this article, we choose TAMER as the foundation and starting point to investigate the potential of agent learning from human trainer's facial expressions, via interpreting them as human reward, and do not claim it is superior to other methods that learn from human evaluative feedback.} 

%Moreover, instead of converting feedback signals into evaluative rewards, Loftin et al.\ \shortcite{loftin2014strategy,loftin2015learning} explore the possibility of inferring knowledge about the desired behavior from cases where no feedback is provided via interpreting human feedback as categorical feedback strategies, and Griffith et al.\ \shortcite{griffith2013policy} propose %an approach called 
%`policy shaping' by formalizing %the meaning of 
%human feedback as labels on the optimality of actions and using it directly as policy advice.%In contrast, our work focuses on how a trainer's facial expressions will affect an agent's learning from human feedback provided through key presses. This will deepen our understanding of how to design agents to learn efficiently from such signals in the future. 


%Using TAMER as a foundation, Knox et al.\ \cite{knox2012humans} examine how human trainers respond to changes in their perception of the agent and to certain changes in the agent's behavior, while Li et al.\ \cite{li2013using} investigate how informative feedback from the agent affects trainers' behaviors. Knox et al.\ find that the agent can induce the human trainer to give more feedback but with lower performance when the quality of the agent's behavior is deliberately reduced whenever the rate of human feedback decreases. Li et al.\ show that more and higher quality feedback is elicited from the trainers when the agent's past and present performance is displayed to the trainer. 

%The approach of Knox et al.\ investigates how agent's task-focused behavior affects trainer's training behavior. However, the approach of Li et al.\ suggests that the agent should also provide information about its learning process to the trainer. Ultimately, we believe that it will be helpful for facilitating the interaction between the trainer and the agent if the agent provides information (such as facial expressions, body language, and gaze behavior) to indicate something about its learning state and solicit feedback from a human \cite{thomaz2005real, thomaz2006transparency, chao2010transparent}. 

%However, as an early step towards this goal, we concentrate in this work on analyzing how agent's sharing socially derived closely related competitive information can influence the trainer's behavior and the relationship between the use of feedback vial facial expressions compared to key presses.

%While the work mentioned above interprets human feedback as a numeric reward, Loftin et al.\ \cite{loftin2014strategy,loftin2014learning} interpreted human feedback as categorical feedback strategies that depend both on the behavior the trainer is trying to teach and the trainer's teaching strategy. They inferred knowledge about the desired behavior from cases where no feedback is provided and showed that their algorithms learn faster than algorithms that treat the feedback as numeric reward. In contrast, our work focuses on how a trainer's facial expressions will affect an agent's learning from human feedback provided through key presses. This will deepen our understanding of how to design agents to learn efficiently from such signals in the future. 

\subsection{Learning from Facial Expressions} %\HH{Why do you emphasis 'affective'? movements of the face do not have to be just affective.}%Affective Facial Expression}

Emotions including expression, %facial expressions \sw{Facial expresssions are not emotions...}, 
motivation, feelings etc., play an important role in information processing, behavior, decision-making and learning in social animals, especially humans \cite{scherer2001appraisal,picard2004affective,frijda2000beliefs,berridge2003pleasures}. %Affect refers to the direction of an emotional state (including facial expressions, motivation, feelings etc.), which can be either positive or negative. 
Much research has been done on the role of emotion in learning. Some classic works on affect, i.e., the direction of an emotional state, emphasize cognitive and information processing aspects in a way that can be encoded into machine-based rules % and studied in a \HH{'learning interaction'? this sentence is not quite clear...}learning interaction 
\cite{picard2004affective,ortony1990cognitive,csikszentmihalyi1991flow}. 
\textcolor{blue}{However, little work has been done to investigate the relation between emotion and learning with computational models especially with reinforcement learning as context, except \cite{gadanho2003learning,broekens2007emotion,leite2011modelling,veeriah2016face,gordon2016affective}.}

Gadanho used an emotion system with capabilities analogous to those of natural emotions to calculate a well-being value that was used as reinforcement by Q-learning to learn behavior selection, i.e., to decide when to switch and reinforce behavior \cite{gadanho2003learning}. %\HH{UNCLEAR: taking them as social human input, i.e. deciding when to switch and reinforce behavior}. 

\textcolor{blue}{Broekens examine the relationship between emotion, adaptation and reinforcement learning by taking human's real emotional expressions as social reinforcement \cite{broekens2007emotion}. Their results show that affective facial expressions facilitate robot learning significantly faster compared to a robot trained without social reinforcement.  %\HH{it's better to write this in past tense when referring to what people did before} 
However, in their work, the social reinforcement is simply added to the environmental reward to form a composite reinforcement. Moreover, %only fear, happy and neutral are taken as prototypes of 
affective facial expressions are mapped to a predefined fixed numeric as social reinforcement. In addition, a mechanism with 9 stickers on the face were used to help recognize facial expressions. By contrast, our work tries to build a model with data collected from 498 people to predict the human trainer's feedback based on her facial expressions during the time of giving keypress feedback without any physical help to recognize them. Moreover, we test %the potential of 
agent's learning from these predictive feedback %myopically by taking them as implicit human reward 
without taking the environmental reward into account.}%investigates the relationship between the use of feedback via facial expressions compared to key presses and studies how the agent can learn from affective facial expressions and human reward without taking environmental reward into account. %our work is in a much more natural setting close to the real-world \HH{why do you say that? elaborate.} and we are the \HH{Let's de-emphasis the correlation between expressions and human reward and emphasis more 'the relationship between the use of feedback via facial expressions compared to key presses'} first to investigate the correlation between the affective facial expressions and the human reward and study how the agent can learn from affective facial expression and human reward without taking the environmental reward into account. 

\textcolor{blue}{Recently, Veeriah et al. \cite{veeriah2016face} propose a method---face valuing, with which an agent can learn how to perform a task according to a user's preference from facial expressions. Specifically, face valuing learns a value function that maps facial features extracted from a camera image to expected future reward. % adapt to the user's preferences pertaining to a task by learning to perceive a value of its behavior from the human user's facial expressions. 
Their preliminary results with a single user suggest that an agent can quickly adapt to a user's changing preferences %by learning a value function that maps facial features extracted from a camera image to expected future reward 
and reduce the amount of explicit feedback required to complete a grip selection task. 
%In their experiments with a single well-trained user and the user was asked to give clear clues to express pleasure or displeasure of the agent's action, 
The motivation of `face valuing' to learn from facial expressions is similar to ours. However, in their experiments, the user is well-trained and asked to give clear clues to express pleasure or displeasure of the agent's action. In our paper, we collect the data of facial expressions and keypress feedback from 498 ordinary people and build a model to predict the human trainer's evaluative feedback based on her facial expressions during the time of giving keypress feedback. Moreover, while `face valuing' interprets facial expressions as human reward and seeks the largest accumulated discounted human reward, in our work, we investigate the potential of agent's learning from facial expressions by interpreting them as immediate human reward. %environmental reward and learns a value function non-myopically from them given by a well-trained user, in our work, we investigate the potential of agent's learning from facial expressions by interpreting them as human reward. %Specifically, we collect the data of facial expressions and keypress feedback from 498 ordinary people and build a model to predict the human trainer's evaluative feedback based on her facial expressions during the time of giving keypress feedback. 
%We test whether the agent can learn from these predictive feedback myopically by taking them as implicit human reward.} %the relationship between the use of feedback via facial expressions compared to key presses and studies how the agent can learn from affective facial expressions and human reward without taking environmental reward into account. 
}

\textcolor{blue}{In addition, Peeled et al. \cite{peled2013predicting} propose a method for predicting people's strategic decisions based on their facial expressions. Their experiment is conducted in a controlled experiment with 22 computer science students. They ask the participants to play several games and record videos of the whole process. At the same time, they log the participants' decisions throughout the games. %In their experiment in a controlled environment, they record video of only 22 participants who are all computer science students and play several games, and concurrently logged their decisions throughout the games. 
The video snippet of the participants' faces prior to their decisions is represented and served as input to a classifier that is trained to predict the participant's decision. Their results show that their method outperforms standard SVM as well as humans in predicting subjects' strategic decisions.} 

\textcolor{blue}{Gordon et al. \cite{gordon2016affective} develop an integrated system with a fully autonomous social robotic learning companion for affective child-robot tutoring. They measure children's valence and engagement via an automatic facial expression analysis system and combine them into a reward signal that is fed into the robot's affective reinforcement learning algorithm. They evaluate their system with 34 children in preschool classrooms for a duration of two months. Their results show the robot can personalize its motivational strategies to each student using verbal and non-verbal actions. However, in their work, the detected valence and engagement are weighted and summed with predefined weights as social reinforcement, while in our work we intend to directly predict the reward value from the detected facial expression.}%experimental paradigm in which children play a second-language learning game on a tablet, in collaboration with a fully autonomous social robotic learning companion. As part of the system, we measured children's valence and engagement via an automatic facial expression analysis system. These signals were combined into a reward signal that fed into the robot?s affective reinforcement learning algorithm. Over several sessions, the robot played the game and personalized its motivational strategies (using verbal and non-verbal actions) to each student. We evaluated this system with 34 children in preschool classrooms for a duration of two months. We saw that (1) children learned new words from the repeated tutoring sessions, (2) the affective policy personalized to students over the duration of the study, and (3) students who interacted with a robot that personalized its affective feedback strategy showed a significant increase in valence, as compared to students who interacted with a non-personalizing robot. This integrated system of tablet-based educational content, affective sensing, affective policy learning, and an autonomous social robot holds great promise for a more comprehensive approach to personalized tutoring.}

\textcolor{blue}{To endow a chess companion robot for children with empathic capabilities, Leite et al. \cite{leite2011modelling} use a multimodal framework to model the user's affective states and allow the robot to adapt its empathic responses to the particular preferences of the child who is interacting with it. They combine visual and task-related features to measure the user's valence of feeling. The change of valence before and after the robot taking the empathic strategy is calculated as rewards for a multi-armed bandit reinforcement learning algorithm. Their preliminary study with 40 children show that robot's empathic behavior has a positive effect on users.} %present a scenario where a social robot acts as a chess companion for children, and describe our current efforts towards endowing such robot with empathic capabilities. A multimodal framework for modeling some of the user's affective states that combines visual and task-related features is presented. Using this model of the user, we personalize the learning environment by adapting the robot's empathic responses to the particular preferences of the child who is interacting with the robot.}

\textcolor{blue}{Our preliminary work on this topic was presented in \cite{li2015large,li2016towards}. This article significantly extends upon our initial work by providing a more extensive analysis of participants' facial feedback and testing the potential of agent learning from them. Our results show the possibility of an agent learning from facial expressions via interpreting them as %implicit 
evaluative feedback. %, though the performance is still worse than from explicit feedback probably because of the low predicted accuracy. 
To our knowledge, it is the first time facial expressions %, or any kind of implicit feedback 
has been shown to work in TAMER, opening the door to a much greater potential for agent learning from human reward.}

\section{Background}
\label{sec:bg}
%Text with citations \cite{RefB} and \cite{RefJ}.
\textcolor{blue}{This section briefly introduces interactive reinforcement learning, technical details on TAMER framework and the Infinite Mario testing domain used in our experiment.}

\subsection{\textcolor{blue}{Interactive Reinforcement Learning}}
\label{sec:irl}

\textcolor{blue}{In traditional reinforcement learning \cite{sutton1998reinforcement,kaelbling1996reinforcement}, the agent learns from rewards provided by a predefined reward function not a human user. %Inspired by potential-based reward shaping \cite{ng1999policy}, 
Different from traditional reinforcement learning, interactive reinforcement learning (Interactive RL) was developed to allow  %also called interactive reinforcement learning, %to facilitate the agent learning from a human observer especially non-experts in agent design and speed up the agent learning at the same time, 
an ordinary human user %human evaluative feedback is used %as a supplemental reward 
to shape the agent learner by providing evaluative feedback \cite{thomaz2008teachable,knox2009interactively,tenorio2010dynamic,loftin2015learning,macglashan2017interactive}.  %\footnote{These methods regarding shaping with human-delivered reward signal shares the same mechanism with potential-based reward shaping, since they also work by directly manipulating the reward function. Therefore, shaping with human-delivered reward through directly manipulating the reward function is also called reward shaping. But in this chapter, we divide the categories based on the source of shaping reward. Therefore, they fall into the category of interactive shaping, as in Section \ref{background:interactive}.} 
The objectives of Interactive RL are to facilitate the agent to learn from a non-expert human user in agent design and even programming, and use the human's knowledge to speed up the agent learning. %However, since the shaping reward is provided by a human trainer not a potential function, the optimality of the optimal policy in the task can not be guaranteed. 
%Since the evaluative feedback is provided by a human teacher not a pre-defined reward function, 
%In the past decade, researchers have developed many algorithms of human-centered reinforcement learning based on different interpretations of human evaluative feedback. %
In interactive reinforcement learning, based on the observed state in the environment, the agent will take an action. Then the human teacher who observed the agent's behavior will evaluate the quality of agent's action based on her knowledge, as shown in Figure \ref{humanrl}. The evaluation is used as feedback for the agent to update the learned behaivor. Therefore, the agent's optimal behavior is decided by the evaluation provided by the human teacher. %every time the agent takes an action in a state, the observing human teacher can provide evaluative feedback which tells whether the selected action is right or wrong based on the teacher's knowledge, as shown in Figure \ref{humanrl}. The agent then uses the evaluative feedback to update its policy. Therefore, it is the human evaluative feedback that decides the agent's behavior.
\begin{figure} [htb]
\centering
%\vspace{-0.2cm}
\includegraphics[width=2.5in]{humanrl.pdf}
%\vspace{-0.4cm}
\caption{Interaction in the interactive reinforcement learning framework.}
\label{humanrl}
%\vspace{-0.2cm}
\end{figure}
}

\textcolor{blue}{As in traditional reinforcement learning (RL), an interactive RL agent learns to make sequential decisions in a task, represented by a policy deciding the action to be taken by the agent in an environmental state. %how to perform a sequential decision task, i.e., a policy that decides which action to take in a state of the environment the agent encounters. %In RL, 
A sequential decision task is modeled as a Markov decision process (MDP), denoted by a tuple \{$S$, $A$, $T$, $R$, $\gamma$\}. In MDP, time is divided into discrete time steps, and $S$ is a set of states in the environment that can be encountered by the agent and $A$ is a set of actions that the agent can perform. At each time step $t$, the agent observes the state of the environment, $s_{t}$ $\in$ $S$. Based on the observation, the agent will take an action $a_{t}$ $\in$ $A$. The experienced state-action pair will take the agent into a new state $s_{t+1}$ in the environment, decided by a transition function $T: S \times A \times S$, which tells the probability of the agent transitioning to one state based on the action selection in a given state, $T(s_{t},a_{t},s_{t+1}) = Pr(s_{t+1}|s_{t}, a_{t})$.
%represents a set of all possible states and $A$ represents a set of all possible actions. Time is divided into discrete time steps. At each time step $t$, the agent receives a representation of the environmental state, $s_{t}$ $\in$ $S$, takes an action $a_{t}$ $\in$ $A$ that results in the next %affects the state of the environment $s_{t+1}$. One time step later, as a consequence of the action $a_{t}$ taken based on the current state $s_{t}$, 
The agent will receive an evaluative feedback $r_{t+1}$, provided by the human observer by evaluating the quality of the action selection based on her knowledge. That is to say, there is no predefined reward function in interactive RL---$R : S \times A \times S \rightarrow \Re$, which decides a numeric reward value at each time step based on the %on the basis of the current state-action pair 
current state, action chosen and the resultant next state. Instead, the reward function is in the human teacher's mind.}
%The probability of the next state $s_{t+1}$ that the agent will experience is decided by the transition function $T: S \times A \times S$, which describes the probability of transitioning from one state to another given a specific action, $T(s_{t},a_{t},s_{t+1}) = Pr(s_{t+1}|s_{t}, a_{t})$. %$\gamma$, the discount factor usually $0 \leq \gamma \leq 1$, determines the present value of future rewards: a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. 
%Figure \ref{StandardReinforcementLearning} depicts an agent's learning by interacting with the environment in a standard reinforcement learning framework. }


\textcolor{blue}{The agent's learned behavior is described as a $\it{policy}$, $\pi : S \times A$, where $\pi(s,a) = Pr(a_{t}=a|s_{t}=s)$ is the probability of selecting a possible action $a$ $\in$ $A$ in a state $s$. The goal of the agent is to maximize the accumulated discounted reward %return it 
the agent receives, denoted as $\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1}$ at time step $t$, where $\gamma$ is the discount factor (usually $0 \leq \gamma < 1$). $\gamma$ determines the present value of rewards received in the future: a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. The return for a policy $\pi$ is denoted as $\sum_{k=0}^{\infty}\gamma^{k}R(s_{t+k}, \pi(s_{t+k}), s_{t+k+1})$.  There are usually two associated value functions for each learned policy $\pi$. One is the $state$-$value$ $function$, referred to as the value of a state,  $V^{\pi}(s)$, which is the expected return when an agent starts in a state $s$ and follows a $policy$ $\pi$ thereafter, where
\begin{equation}
    V^{\pi}(s) = E_{\pi}\left[ \sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1} | s_{t} = s\right].
\end{equation}
}

\textcolor{blue}{Similarly, another value function is the $action$-$value$ $function$, referred to as the value of a state-action pair,  $Q^{\pi}(s,a)$,  which is the expected return after taking an action $a$ in a state $s$, and thereafter following a $policy$ $\pi$, where
\begin{equation}
    Q^{\pi}(s, a) = E_{\pi}\left[ \sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1} | s_{t} = s, a_{t} = a\right].
\end{equation}
}

\textcolor{blue}{For each MDP, there exists a set of optimal policies $\pi^{\ast}$, which share the same optimal $state$-$value$ $function$, $V^{\ast}$, defined as $V^{\ast}(s) = \max_{\pi}V^{\pi}(s)$, %Optimal policies also share the same optimal 
and $action$-$value$ $function$, $Q^{\ast}$, defined as $Q^{\ast}(s, a) = \max_{\pi}Q^{\pi}(s, a)$. The goal of the agent is to learn an optimal policy $\pi^{\ast}$ from its interaction with the human teacher.}

\subsection{TAMER Framework}
\label{sec:tamer}

\textcolor{blue}{In this article, we use the TAMER framework \cite{knox2009interactively} as the agent's learning algorithm. The TAMER framework was built for a variant of the Markov decision process (MDP), a model of se\-quen\-tial de\-ci\-sion-making addressed via dynamic programming \cite{howard1960dynamic} and reinforcement learning \cite{sutton1998reinforcement}. In the TAMER framework, there is no reward function encoded before learning. An agent implemented according to TAMER learns from real-time evaluations of its behavior, provided by a human teacher who observes the agent. These evaluations are taken as human reward signals. Therefore, TAMER is a typical interactive reinforcement learning method.}

\textcolor{blue}{Knox and Stone first proposed the original TAMER framework which learns the reward function and selects actions with it \cite{knox2009interactively}, then proposed VI-TAMER which learns a value function from the learned human reward function via value iteration and selects actions with the value function \cite{knox2015framing}. In the original TAMER framework, the agent learns myopically from human reward, i.e., only taking the immediate reward into account by setting the discount factor $\gamma$ to 0. In VI-TAMER, the discount factor $\gamma$ is set close to 1, i.e., the agent seeks the largest accumulated discounted human reward, which is the same as in traditional reinforcement learning. Therefore, the original TAMER is equivalent to VI-TAMER when the discount factor $\gamma$ is set to 0. %In this paper, we set the discount factor $\gamma$ to 0. In such a case, the agent learns myopically from human reward, i.e., only taking the immediate reward into account. 
In such case, the learned value function in VI-TAMER is equivalent to the learned human reward function.}

\textcolor{blue}{In this article, we rephrase TAMER as a general model-based method for interactive reinforcement agent learning from human reward, as shown in Figure \ref{tamer}. In this case, the TAMER agent learns a model of the human reward and then uses the learned human reward function to learn a value function model. The TAMER agent will select actions with the value function model to get the most accumulated discounted human reward. Figure \ref {tamer} shows the diagram of agent learning in the TAMER framework.}

%The TAMER framework was built for a variant of the Markov decision process (MDP), a model of se\-quen\-tial de\-ci\-sion-making addressed via dynamic programming \cite{howard1960dynamic} and reinforcement learning \cite{sutton1998reinforcement}. In the TAMER framework, there is no reward function encoded before learning and a TAMER agent learns from a human trainer's real-time evaluation of its behaviors. The agent interprets this evaluation as \emph{human reward}, creates a predictive model of it, and selects actions it predicts will elicit the most human reward. Figure \ref {tamer} shows the diagram of agent learning in the TAMER framework.

%The TAMER agent strives to maximize the immediate reward, %caused by its action, 
%which contrasts with traditional reinforcement learning, in which the agent seeks the largest discounted sum of future rewards. There are two reasons why an agent can learn to perform tasks from a myopic reward value. Firstly, the human reward can be delivered with small delay, which is the time it takes for a trainer to evaluate the agent's action and deliver her feedback. \textcolor{blue}{Secondly, the evaluation provided by a human trainer is interpreted as a judgement of the agent's behavior and the human trainer already takes a long-term consequence of the agent's behavior in mind when she is providing the evaluation. %Secondly, the evaluation provided by a human trainer carries a judgment of the behavior itself with a mental model of its long-term consequences in mind.
%} Until recently, \textcolor{blue}{general myopia was a feature of many ???%all 
%algorithms involving learning from human feedback} and has received empirical support~\cite{knox2015framing}.


\begin{figure} [ht]
\centering
\includegraphics[width=2.7in]{TAMER.pdf}%HumanAgentEnvConceptualDiagram}
\caption{Agent learning in the TAMER framework (modified from \protect\cite{knox2012learning}). }
\label{tamer}
%\vspace{-4mm}
\end{figure}

\textcolor{blue}{Specifically, in TAMER, the human teacher observes the agent's behavior and can give reward corresponding to its quality. There are four key modules for an agent learning with TAMER. The first one is to learn a predictive model of human reward. Specifically, the TAMER agent learns a function:
\begin{equation}
 \hat{R}_{H}(s, a) = \vec{w}^{\mathrm{T}}\Phi(s,a), 
\end{equation}
where $\vec{w} = (w_{0}, ..., w_{m-1})^{\mathrm{T}}$ is the parameter vector, and $\Phi(\vec{x}) = (\phi_{0}(\vec{x}), ..., \phi_{m-1}(\vec{x}))^{\mathrm{T}}$ are the basis functions, and $m$ is the total number of parameters.  $\hat{R}_{H}(s, a)$ is a function for approximating the expectation of human rewards received in the interaction experience, ${R}_{H}: S \times A \rightarrow \Re$. }

\textcolor{blue}{Since it takes time for the human teacher to assess the agent's behavior and deliver her feedback, the agent is uncertain about which time steps the human reward is targeting at. The second module is the credit assigner to deal with the time delay of human reward caused by evaluation of agent's behavior and delivering it. TAMER uses a credit assignment technique to deal with the delay of human reward and multiple rewards for a single time step. Specifically, inspired by the research on the delay of human's response in visual searching tasks of different complexities \cite{hockley1984analysis}, TAMER defines a probability density function to estimate the probability of the teacher's feedback delay. This probability density function provides the probability that the feedback occurs within any specific time interval and is used to calculate the probability (i.e. the credit) that a single reward signal is targeting a single time step. If a probability density function $f(t)$ is used to define the delay of the human reward, then at the current time step $t$, the credit for each previous time step $t$-$k$ is computed as:
\begin{equation}
c_{t-k} = \int_{t-k-1}^{t-k} f(x) dx.
\label{credit}
\end{equation}
}

\textcolor{blue}{If the human teacher gives multiple rewards, the label $h$ for each previous time step (state-action pair) is the sum of all credits calculated with each human reward using Equation \ref{credit}. %As indicated in \cite{knox2012learning}, the agent is still allowed to learn when no feedback is received with the credit assign technique.
The TAMER agent uses the calculated label and state-action pair as a supervised learning sample to learn a human reward model --- $\hat{R}_{H}(s, a)$ by updating its parameters, e.g., with incremental gradient descent. If at any time step $t$ the human reward label $h$ received by the agent is not 0, temporal difference error $\delta_{t} $ is calculated as
\begin{equation}
\begin{aligned}
\delta_{t} &= h - \hat{R}_{H}(s, a) \\
&= h - \vec{w}^{\mathrm{T}}\Phi(s_{t},a_{t}).
\end{aligned}
\label{herror}
\end{equation}
}

\textcolor{blue}{Based on the gradient of least square, the parameter of $\hat{R}_{H}(s, a)$ is updated with incremental gradient descent:
\begin{equation}
\begin{aligned}
\vec{w}_{t+1} &= \vec{w}_{t} - \alpha\nabla_{\vec{w}} \frac{1}{2} \left\{h - \hat{R}_{H}(s_{t}, a_{t})\right\}^{2} \\
%&= \vec{w}_{t} - \alpha \nabla_{\vec{w}}\frac{1}{2} \left\{h - \vec{w}^{\mathrm{T}}\Phi(s_{t},a_{t})\right\}^{2} \\
%&= \vec{w}_{t} + \alpha \left\{h - \vec{w}^{\mathrm{T}}\Phi(s_{t},a_{t})\right\} \Phi(s_{t},a_{t})\\
&= \vec{w}_{t} + \alpha \delta_{t}\Phi(s_{t},a_{t}),
\end{aligned}
\label{rupdate}
\end{equation}
where $\alpha$ is the learning rate.}

\textcolor{blue}{The third one is the value function module. The TAMER agent learns a state value function model --- $V(s)$ or an action value function model --- $Q(s,a)$ from the learned human reward function $\hat{R}_{H}(s, a)$. At each time step, the agent will update the value function as:
\begin{equation}
Q(s,a) \leftarrow \hat{R}_{H}(s, a)+\gamma \sum_{s' \in S}T(s,a,s') \times max_{a'}Q(s',a'), 
\end{equation}
or 
\begin{equation}
V(s) \leftarrow max_{a}[\hat{R}_{H}(s, a)+\gamma \sum_{s' \in S}T(s,a,s') V(s')].
\end{equation}
where $T(s,a,s')$ is the transition function, $s$ and $a$ are current state and action, $s'$ and $a'$ are the next state and action, .
}

%\textcolor{blue}{The value function model update with learning $\hat{R}_{H}(s, a)$ can also be combined with planning methods such as value iteration, Monte Carlo Tree Search etc. In such cases, the agent can use simulated experience to speed up its learning.}

\textcolor{blue}{The fourth module is the action selector with the predictive value function model. As a traditional RL agent which seeks the largest discounted accumulated future rewards, the TAMER agent can also seek the largest accumulated discounted human reward by greedily selecting the action with the largest value, as: %A TAMER agent maximizes the human reward on its immediate action, i.e., the discount factor is 0, while a traditional RL agent seeks the largest discounted accumulated future rewards. Therefore, in each state $s$, the TAMER agent will myopically select the action with the largest estimated expected human reward, 
\begin{equation}
a \leftarrow \argmax_{a}Q(s, a),
\end{equation}
or 
\begin{equation}
a \leftarrow \argmax_{a}[\hat{R}_{H}(s, a)+\sum_{s' \in S}T(s,a,s')V(s')]. 
\end{equation}
}

\textcolor{blue}{The TAMER agent learns by repeatedly taking an action, sensing reward, and updating the predictive model $\hat{R}_{H}$ and corresponding value function model. The trainer observes and evaluates the agent's behavior. She can  give reward %. the human trainer provides feedback 
by pressing two buttons on the keyboard, which are assigned to the agent's most recent actions. Each press of the two buttons is mapped to a numeric reward of -1 or +1 respectively. In our experiment, we set the discount factor $\gamma$ to 0. Then the learned value function in TAMER is equivalent to the learned human reward function $\hat{R}_{H}$. 
And the action selector chooses actions with $\hat{R}_{H}$. Note that unlike \cite{knox2012learning}, when no feedback is received from the trainer, learning is suspended until the next feedback instance is received. %Figure $\ref{tamer}$ depicts a TAMER agent learning from evaluative feedback provided by a human teacher. 
}


%Specifically, it learns a function $\hat{R}_{H}(s, a)$ that approximates the expected human reward conditioned on the current state and action, $R_{H}: S \times A \rightarrow \Re$. Given a state $s$, the agent myopically chooses the action with the largest estimated expected reward, $\argmax_{a}\hat{R}_{H}(s, a)$. The trainer observes the agent's behavior and can give reward corresponding to its quality. The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the estimate of $\hat{R}_{H}(s, a)$. %In this paper, the update is performed by incremental gradient descent; i.e., the weights of the function approximator specifying $\hat{R}_{H}(s, a)$ are updated to reduce the error $|r - \hat{R}_{H}(s, a)|$, where $r$ is the sum of reward instances observed shortly after taking action $a$ in state $s$. 


%The TAMER agent strives to maximize the immediate reward, %caused by its action, 
%which contrasts with traditional reinforcement learning, in which the agent seeks the largest discounted sum of future rewards. Until recently, general myopia was a feature of all algorithms involving learning from human feedback and has received empirical support~\cite{knox2015framing}.
%The trainer observes and evaluates the agent's behavior and can give reward. 


%The intuition for why an agent {\it can} learn to perform tasks using such a myopic valuation of reward is that human feedback can generally be delivered with small delay---the time it takes for the trainer to assess the agent's behavior and deliver feedback---and the evaluation that creates a trainer's reward signal carries an assessment of the behavior itself, with a model of its long-term consequences in mind. Until recently~\cite{knox2013learning}, general myopia was a feature of all algorithms involving learning from human feedback and has received empirical support~\cite{knoxreinforcement}. Built to solve a variant of a Markov decision processes, (i.e., a specification of a sequential decision-making problem commonly addressed through reinforcement learning \cite{sutton1998reinforcement}) in which there is no reward function encoded before learning, the TAMER agent learns a function $\hat{H}(s, a)$ that approximates the expectation of experienced human reward, $H: S \times A \rightarrow \Re$. Given a state $s$, the agent myopically chooses the action with the largest estimated expected reward, $\argmax_{a}\hat{H}(s, a)$. The trainer observes the agent's behavior and can give reward corresponding to its quality.

%\textcolor{blue}{The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then used it as a supervised learning sample to update the estimate of $\hat{H}(s, a)$. In this paper, the update is performed by incremental gradient descent; i.e., the weights of the function approximator specifying $\hat{H}(s, a)$ are updated to reduce the error $|r - \hat{H}(s, a)|$, where $r$ is the sum of reward instances observed shortly after taking action $a$ in state $s$. ???}

%In this paper, \HH{Can we claim novelty here? i.e. showing that TAMER can be used to learn a model tree? In reality, we still had to be careful with the types of input features used... however, this was not neccessarily a straight-forward task.}the TAMER agent learns a \emph{model tree} \cite{quinlan1992learning} that constructs a tree-based \HH{this part doesn't make sense to me. Which part of the model is piecewise-linear? the update update of the tree node, parameters? For completeness, I think it would be a good idea to elaborate on the method here in a bit more detail. It's not clear here also why we had to use this method and not the existing TAMER setup. Wouldn't this paragraph be better placed and elaborated upon in the infinit Mario part? } piecewise-linear model to estimate $\hat{R}_{H}(s, a)$.  The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the model tree by the divide-and-conquer method.  %In this paper, the TAMER agent learns a update is performed by a model tree \cite{quinlan1992learning}; i.e., the weights of the function approximator specifying $\hat{H}(s, a)$ are updated to reduce the error $|r - \hat{H}(s, a)|$, where $r$ is the sum of reward instances observed shortly after taking action $a$ in state $s$.

%Specifically, in the TAMER framework, the agent learns a function $\hat{R}_{H}(s, a)$ that approximates the expected human reward conditioned on the current state and action, $R_{H}: S \times A \rightarrow \Re$. Given a state $s$, the agent myopically chooses the action with the largest estimated expected reward, $\argmax_{a}\hat{R}_{H}(s, a)$. The trainer observes and evaluates the agent's behavior and can give reward %. the human trainer provides feedback 
%by pressing two buttons on the keyboard, which are assigned to the agent's most recent action. Each press of the two buttons is mapped to a numeric reward of -1 or +1 respectively. The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the estimate of $\hat{R}_{H}(s, a)$.

%Inspired by the research on the delay of human's response in visual searching tasks of different complexities \cite{hockley1984analysis}, TAMER defines a probability density function to estimate the probability of the teacher's feedback delay. This probability density function provides the probability that the feedback occurs within any specific time interval and is used to calculate the probability (i.e. the credit) that a single reward signal is targeting a single time step. The human trainer can strengthen her feedback by pressing either button multiple times and the label for a sample is calculated as a delay-weighted aggregate reward based on the probability that a human reward signal targets a specific time step \cite{knox2012learning}.  %The reward value was always bounded between -4 and +4. %(up to $\pm 4$). 
%The interface flashes blue or red for positive or negative reward, respectively, to inform the trainer when she is giving feedback. 
%Then the agent learns a function $\hat{R}_{H}(s, a)$ that approximates the expected human reward conditioned on the current state and action, $R_{H}: S \times A \rightarrow \Re$. Given a state $s$, the agent myopically chooses the action with the largest estimated expected reward, $\argmax_{a}\hat{R}_{H}(s, a)$. The trainer observes the agent's behavior and can give further reward corresponding to its quality. The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the estimate of $\hat{R}_{H}(s, a)$.


%A TAMER agent learns by repeatedly taking an action, sensing reward, and updating $\hat{R}_{H}$. Note that unlike \cite{knox2012learning}, when no feedback is received from the trainer, learning is suspended until the next feedback instance is received. 

%\vspace{-1mm}
%\HH{I'm not sure if this belongs in the background section... It mixes our implementation in with existing implementations. It undersells the fact that porting Infinite mario into TAMER was not straightforward. I think it might make more sense to put this in a different section as a description of the novel proposed methodology that is used to evaluate our experimental conditions. The clarity and narrative in this part is also confusing. You really need to explain the concepts step by step so it's clear which are the states, which paossible actions there are...currently, new knowledge about the states does not appear all at once but trickles in a bit as a time almost incidentally as you're describing othe parts of the set up. It's really not easy to get a mental picture of what's happening.}


\subsection{Infinite Mario Domain}
%\vspace{-1mm}
\label{sec:domain}

Super Mario Bros is an extremely popular video game, %and has been ranked first on the ``Top 100 Video Games of All Time''\footnote{\url{http://en.wikipedia.org/wiki/Super\_Mario\_Bros.}}, 
making it an excellent platform for investigating how humans interact with agents that are learning from them. %We used 
To establish the generalizability of TAMER to more complex domains, and to make the experiment appealing to trainers of all ages, \textcolor{blue}{we implemented TAMER in the Infinite Mario domain from the Reinforcement Learning Competition \cite{whiteson2010reinforcement,dimitrakakis2014reinforcement}. The Infinite Mario domain was adapted from the classic Super Mario Bros video game.} %\HH{To establish the generalisability of TAMER to more complex domains, and to make the experiment appealing to trainers of all ages, we implemented}

%Infinite Mario is a complex and challenging domain from the Reinforcement Learning Competition \cite{whiteson2010reinforcement} that was adapted from the classic Super Mario Bros video game. 
\textcolor{blue}{In Infinite Mario, the Mario avatar must move towards the right of the screen as fast as possible %to the right as soon as possible 
and at the same time collect as many points as possible. To facilitate comparisons of TAMER with other learning methods that have been applied to this domain, we used the standard scoring mechanism that was established for the Reinforcement Learning Competition. The standard scoring mechanism gives positive reward for killing a monster (+1), grabbling a coin (+1), and finishing the level (+100).} It gives negative points for dying (-10) and for each time step that passes (-0.01). The actions available for Mario %correspond to the buttons on a standard Nintendo controller, which 
are (left, right, no direction), (not jumping, jumping) and (not sprinting, sprinting), resulting in 12 different combined actions for the agent at every time step. The state space is quite complex, as Mario observes a 16 $\times$ 21 matrix of tiles, each of which has 14 possible values. 

\textcolor{blue}{To reduce the state space, in our TAMER implementation we take each visible enemy (i.e. monster) and each tile within a 8 $\times$ 8 region around Mario as one state feature. The most salient features of the observations will be extracted as state representation. 
%and filter the top two small states %consider an  as his state. 
%and filter two The state representation includes properties of Mario and two picked tiles within the 8 $\times$ 8 region 
%by ranking all small states based on a priority of whether it is a pit, monster, block and distance. %\sw{I don't understand this: are you describing the state representation or the actions that are available?} 
%\HH{How is the distance relevant in this case? Isn't it always 1 tile away?} 
%\sw{I still don't understand how the agent can ``search'' the state space.  When does this searching occur?  How does the agent decide where to search? What does it have to do with the state represention?}
For each state feature, a number of properties are defined, including whether it is a: 
\begin{itemize}
\item pit, 
\item enemy, 
\item mushroom, 
\item flower, 
\item coin, 
\item smashable block,
\item question block, 
\end{itemize}
and the distance ($x$---horizontal direction, $y$---vertical direction and Euclidean distances) from Mario. 
We filter and select the top two state features %consider an  as his state. 
%and filter two The state representation includes properties of Mario and two picked tiles within the 8 $\times$ 8 region 
by ranking all state features based on a priority of whether it is a pit, an entity (a monster, mushroom, flower or fireball), a block and the distance.
The state representation includes the properties of the selected two state features and the properties of Mario.  
The properties of Mario include whether it is at the right of a wall and the speed of it ($x$-speed and $y$-speed). Thus, the feature vector $\Theta$ for the state representation is 
\begin{equation}
\label{eqn:sr}
 \Theta = [\phi_{1}, \phi_{2}, \phi_{M}],
\end{equation}
where $\phi_{1}$ and $\phi_{2}$ are two vectors for the two selected state features with each consisting of the above 10 properties, and $\phi_{M}$ is a vector consisting of the properties of Mario.}

%To reduce the state space, in our TAMER implementation we consider an 8 $\times$ 8 region of tiles around Mario as his state. 
%\textcolor{blue}{The state representation includes properties of Mario and two picked tiles within the 8 $\times$ 8 region ranked by priority of whether it is a pit, monster, block and distance. %\sw{I don't understand this: are you describing the state representation or the actions that are available?} 
%%\HH{How is the distance relevant in this case? Isn't it always 1 tile away?} 
%%\sw{I still don't understand how the agent can ``search'' the state space.  When does this searching occur?  How does the agent decide where to search? What does it have to do with the state represention?}
%For the two selected tiles, the properties include whether it is a pit, monster, mushroom, flower, coin, smashable block or question block and the distance from Mario. }The properties of Mario include whether it is at the right of a wall and the speed of it.
%%\HH{This gets a bit confusing. You didn't mention mushrooms or flowers before... or whether the block was smashable block, questionmark or coin. Aren't they also part of the state? Or is there a set of possible states for each tile to be in? You describe all this part as if the reader should already know. Given how crucial this part is to the paper and the fact that this is part of the novel implementation, you need to introduce it properly.}
%%and the distance from Mario and whether it is a smashable block, a question mark block or a coin. Apart from these, we also select whether Mario is at the right of a wall and the speed of Mario as features of the state representation %\HH{this is quite confusing. You mentioned only pit, monster, block and distance before as the state representation. And now you say that being speed and walls are also part of the state representation. This was not mentioned before!}. 
%Thus, the feature vector $\Theta$ for the state representation is 
%\begin{equation}
%\label{eqn:sr}
% \Theta = [\phi_{1}, \phi_{2}, \phi_{M}],
%\end{equation}
%where $\phi_{1}$ and $\phi_{2}$ are two vectors for the two selected state features with each consisting of the above properties, and $\phi_{M}$ is a vector consisting of the properties of Mario.

In this article, %\HH{Can we claim novelty here? i.e. showing that TAMER can be used to learn a model tree? In reality, we still had to be careful with the types of input features used... however, this was not neccessarily a straight-forward task.}
the TAMER agent learns a \emph{model tree} \cite{wang1996induction} that constructs a tree-based %\HH{this part doesn't make sense to me. Which part of the model is piecewise-linear? the update update of the tree node, parameters? For completeness, I think it would be a good idea to elaborate on the method here in a bit more detail. It's not clear here also why we had to use this method and not the existing TAMER setup. Wouldn't this paragraph be better placed and elaborated upon in the infinit Mario part? } 
piecewise-linear model to estimate $\hat{R}_{H}(s, a)$.  
The inputs to $\hat{R}_{H}$ are the above state representation %properties of the two selected tiles, features of Mario, 
and the combined action. %\sw{You mean these are the inputs to $H$?  Make this more precise.}
\textcolor{blue}{The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the model tree by the divide-and-conquer method. The model tree can have multivariate linear models at each node with only features tested in the subtree of the current node instead of using all features, analogous to piecewise linear functions. We use model tree because features for state representation are mostly binary and not all features are always relevant. Model tree can select the relevant subset of the features to predict the human reward, thus resulting in more accurate prediction.}

%The inputs to $\hat{R}_{H}$ are the above state representation %properties of the two selected tiles, features of Mario, 
%and the combined action. %\sw{You mean these are the inputs to $H$?  Make this more precise.}
%The TAMER agent takes each observed reward signal as part of a label for the previous $(s,a)$ and then uses it as a supervised learning sample to update the model tree by the divide-and-conquer method.  %In this paper, the TAMER agent learns a update is performed by a model tree \cite{quinlan1992learning}; i.e., the weights of the function approximator specifying $\hat{H}(s, a)$ are updated to reduce the error $|r - \hat{H}(s, a)|$, where $r$ is the sum of reward instances observed shortly after taking action $a$ in state $s$.
\textcolor{blue}{In our study, %this version of the game, 
we have 3 levels (0, 1 and 2) %with seed 121 %\sw{What does this mean?} 
in Infinite Mario domain. %in our study. 
Level 0 is from the Reinforcement Learning Competition generated with seed 121 difficulty 0. Note that the seed is a random integer value that was used by the level generator to generate levels by probabilistically choosing a series of idiomatic pieces of levels and fitting them together \cite{smith2009rhythm}. %Level 0 is the same as in the Reinforcement Learning Competition. %and we designed levels 1 and 2. 
We designed level 1 and 2 based on level 0 %were designed 
with increased difficulties, e.g., increasing the number of monsters, changing the type of monsters, adding one pit, changing the height of walls and length of flat stretches, etc. As in Super Mario Bros, Mario can enter the next level automatically if he finishes one level. The game goes back to level 0 if Mario dies or finishes level 2. A given game ends when Mario dies. }%\HH{You need to add a decription of how levels 1 and 2 are designed to increase difficulty.}

To see whether a TAMER agent can successfully learn to play the game and compare the learning performance with other methods, 
\textcolor{blue}{the first author trained the agent on level 0 % with seed 121 %\HH{I dont' recall any prior mention of the difficulty level or seed. If this is important, then you should also mention this in the experimental set up.} 
in the Infinite Mario Domain for 10 trials with TAMER. }
%Figure \ref{setup&feedback&performance}c shows the average offline performance per episode. 
An episode ends when Mario dies or finishes the level. The policy was frozen and recorded at the end of each episode of training. Then, each recorded policy was tested for 20 games offline. The performance for each episode %in Figure \ref{setup&feedback&performance}c 
was averaged over the 20 games and then over the 10 trials. The result shows that our TAMER agent can achieve 120 points in 12 episodes, while it takes about 500 episodes for a SARSA agent to achieve a similar performance level \cite{taylor2011teaching} and a hierarchical SARSA agent implemented with object-oriented representation about 50 episodes to reach a similar level and 300 episodes to achieve 149.48 points \cite{mohan2011object}, which is almost optimal. Although the TAMER agent does not learn an optimal policy, it can successfully learn a good policy substantially faster than these other methods, making this set-up very suitable for our experiments with members of the public where each training session can only last up to 15 minutes.
 %\HH{It's great that you did this. Very good!} 
%\HH{I wonder if this part would be better described in the experimental set up. This part isn't really a result in itself but a way of showing that we did some serious experimental preparation to verify the efficacy of the set up. }

%\vspace{-4mm}\HH{I think you may need to move this before the experimental conditions. I think it would make more sense to explain the physical set up in the room and where the cameras were. I think you also need to explain important information about how they sat in the room. i.e. they could not see the screens of the people and did not sit facing each other.....etc. You also don't mention what they gave consent for. }

\section{Experimental Setup}
%\vspace{-1mm}
\label{sec:es}

%\HH{consider moving this before the conditions as the conditions are limited to some extent by the conditions of the NEMO science live programme.}

Our user study was conducted in conjunction with the research program of the NEMO science museum in Amsterdam.  This program enables museum visitors to participate as experimental subjects in studies conducted by scientists from nearby universities.  In our experiment, a group of up to four trainers, typically a family or group of friends, trained their own TAMER agents at the same time. In each group, each participant sat at her own table, facing away from the other members and their screens. There was also a camera on the screen in front of the trainer's face. Each participant signed a consent form (with parental consent for children under 18 years old) permitting recording the data and using it for research.
%and was briefed before the experiment about instructions. %\HH{about what?}. 
Then participants in the group were asked to train their own agents for 15 minutes in the same room at the same time. 

Each participant could quit at any time she wanted before the 15 minutes elapsed. Finally, we debriefed the participants and asked for feedback and comments. The experiment was carried out in the local language with English translations available for foreign visitors. \textcolor{blue}{We recorded the training data including state observation, action, human reward, the time of human reward being given, score, the ending time for each time step, and video data of facial expressions and key presses on the keyboard for each trainer during training. Note that one time step corresponds to the execution of an action by the agent. %We recorded the training data including state observations, actions, human rewards and timestamp, scores, timestamp for each time step, and video data of facial expressions and key presses on the keyboard for each trainer during training.
} 
Trainers were not given time to practice before the experiment because we were concerned that they might get tired of expressing facial emotions after the practice.

%to evaluate the conditions we proposed, as described in Section \ref{sec:cons}. 


%\begin{figure}[htb]
%\vspace{-4mm}
%%\hspace{4mm}
%\centering
%%\begin{tabular}{c c c}
%\includegraphics[width=0.7\columnwidth,trim=0 0 0 0, clip]{AgeGenderDistribution}
%%\includegraphics[width=0.4\columnwidth,trim=0 0 0 0, clip]{NumberOfTimeStepsWithFeedback}&
%%\includegraphics[width=0.65\columnwidth,trim=0 0 0 0, clip]{MeanOfflinePerformancePerEpisode} \\
%%a&b&c
%%\end{tabular}
%\vspace{-3mm}
%\caption {Distribution of subjects across age ranges and genders, (F=Female, M=Male).}% b) mean number of time steps with feedback, with and without facial expression and competitiveness and c) mean offline performance per episode trained by one of the authors. Note: black bars in Figure b stand for standard error.}
%\vspace{-4mm}
%\label{demographic}
%\end{figure}

%\HH{I think it is better when describing the actual experimental set up that you do this in the past tense}
Our experiment is a between-subjects study with 561 participants from more than 27 countries participated and %were 
randomly distributed into our four experimental conditions (described below). %i.e., between-subjects study.  %the four conditions described in the next section. %described in previous section. %Section \ref{sec:conditions}. 
Of them, 221 were female and 340 were male respectively, aged from 6 to 72. Figure \ref{demographic} %Figure \ref{demographic} 
shows the distribution of participants across age ranges and genders. %As shown in Figure \ref{setup&feedback&performance}a, %\HH{please do make sure that both gender distributions for different age categories are shown}, %256 (87 females and 169 males) subjects younger than 13 years old, 72 (25 females and 47 males) subjects between 13 and 18 years old, 60 (27 females and 33 males) subjects between 19 and 30 years old, 124 (68 females and 56 males) subjects between 31 and 50 years old, 19 (6 females and 13 males) subjects older than 50 years old and 30 (8 females and 22 males) 
%45.6\% of the subjects were less than 13 years old, 13\% were between 13 and 18 years old, 10.7\%  were between 19 and 30 years old, 22\% were between 31 and 50 years old, 3.4\% were more than 50 years old and  5.3\% subjects did not report their age.
%\footnote{Some subjects did not fill in the age or fill the wrong age in the consent form.} %\HH{Please give distribution across the age ranges and gender as text and also as bar graphs. You have space - the graphs in the paper currently are quite big...when you shrink the figures, you will also need to increase the font size. It is currently far too small.}. 
Data from 63 participants were disregarded: five participants lacked parental consent; three had not played Super Mario Bros before and were unable to judge Mario's behavior; one had an internet connection problem, one quit after only five minutes' training; and the rest did not fully understand the instructions, got stuck and gave feedback randomly by alternating positive and negative feedback in quick succession, or interrupted their family members. %because they did not understand the instructions and gave feedback randomly by pressing the buttons in turn. The choice was made based on the judgment of the experimenter during the experiment and by checking the recorded audio, video, and key presses afterwards. \sw{This still seems really vague to me.} %\HH{You need to elaborate on how this choice was made to show that the selection of these bad data points was performed as objectively as possible.} 
After pruning the data, 498 participants remained: 109 participants in the control condition; 100 in the facial expression condition; 135 in the competitive condition; and 154 in the competitive facial expression condition.

\begin{figure}[tb]
%\vspace{-2mm}
%\hspace{4mm}
\centering
\includegraphics[width=0.55\columnwidth,trim=10 10 10 0, clip]{AgeGenderDistribution} 
%\vspace{-15mm}
\caption{ Demographic information of participants across age ranges and genders in our study. (F=Female, M=Male)}%\HH{Can you also include the gender in the pie chart (this is what I had meant before...?) }}%\sw{``Feedback'' should be lowercase in the graph header.}}
%\vspace{-4mm}
\label{demographic}
\end{figure}

%\begin{table}[htb]
%%\vspace{-2mm}
%\centering
%%\caption{Demographic information of participants in our experiment. }%Distribution of subjects across age ranges and genders.}
%\vspace{2mm}
%\begin{tabular}{ | r | c | c | c |}
%\toprule
%& Under 13 & 13 to 30 & Above 30  \\
%\toprule
%\textbf{Female} & 90 & 57 &74\\
%\hline
%\textbf{Male} & 187 & 81 & 72 \\
%\toprule
%\end{tabular}
%\caption{Demographic information of participants in our experiment. }
%%\vspace{-4mm}
%\label{demographic}
%\end{table}

%\vspace{-1mm}
\section{Experimental Conditions}
%\vspace{-1mm}
\label{sec:cons}

%In this section, we propose two variations on the baseline TAMER interface described above that each display additional information about the agent?s performance history or internal processes. These variations are motivated by the notion that the interaction between the trainer and the agent should be consciously designed to be bidirectional, where the agent gives the trainer informative and/or motivating feedback about its learning process. Our intuition is that such feedback will help keep the trainer?s involvement in the training process and empowers them to offer more useful feedback. More objectively, we hypothesize that doing so will increase the quantity of the trainer?s feedback and improve the agent?s task performance.

In this section, we describe the four conditions we proposed and tested in our experiment. \textcolor{blue}{We investigate how the agent's competitive feedback and telling trainers to use facial expression as an additional channel to train agents affect %We investigate how the agent's competitive feedback---the agent's performance relative to agents trained by other trainers, and telling trainers to use facial expression as an additional channel to train agents affect %the amount of feedback received by the agent, 
the agent's learning performance and trainer's facial expressiveness. Here the agent's competitive feedback means the agent's performance relative to agents trained by other trainers. Based on our previous work in \cite{li2014learning} which show that agent's social competitive feedback can improve the agent's learning, we hypothesize that the agent's competitive feedback will motivate the trainer to train agents better even in a different setting. %, and telling to use facial expression will %reduce the number of keypress feedback received from the trainer and 
%result in worse agent performance as it might distract the trainer from giving high quality keypress feedback. 
In addition, we expect that %both the agent's competitive feedback and 
telling to use facial expression will increase the trainer's facial expressiveness.}

%The interfaces for the control and trick facial expression conditions are replicated from \cite{li2013using}. 
Note that as in the original TAMER, %participants in all four conditions could give positive and negative feedback by pressing buttons on the keyboard to reward or punish the previous action of the agent. Their feedback can be strengthened by pressing the button multiple times. The reward value was always bounded between -4 and +4. 
in all conditions, participants could give positive and negative feedback by pressing buttons on the keyboard to train the agent. Only the keypress signal was used for agent learning and videos of training by participants in all conditions were recorded. 

%\vspace{-1mm}
\subsection{Control Condition} 
%\vspace{-1mm}
The interface for the control condition is the performance-informative interface replicated from \cite{li2016using} and implemented in the Infinite %\HH{ Can you just write the name of the game in full?i.e. Super Mario Brothers}
Mario domain, as shown in Figure \ref{control}. %Apart from the state and action of the agent, which are visible from the game board, the agent's performance over past and current games is shown in a performance window during the training process. The performance window allows the agent to give the trainer %\HH{I would suggest to remove 'motivating'. This is more speculatory than proven...} %motivating 
%feedback about its learning process and keeps the trainer involved in the training process \cite{li2013using}. Trainers in this condition were told to use key presses to train the agent.

%\HH{You must say that video was recorded of the trainer while they trained the agent and that they were informed of this fact. Depending on the ordering of the sections (as I have suggested in the earlier comment), you can either put it here, or in the experimental set up. However, it's important for the experimental design that they knew that they were being recorded with video and that their parents were also aware of this fact wrt. their children. }
%and empowers her to give useful feedback. %\sw{Why?} 

%\begin{figure}[htb]
%\centering
%\begin{tabular}{c c}
%\includegraphics[width=0.5\linewidth]{control} & 
%\includegraphics[width=0.4\columnwidth,height=1.7in]{NumberOfTimeStepsWithFeedbackScoreLevel.pdf}\\
%(a)&(b)
%\end{tabular}
%%\vspace{-4mm}
%\caption{(a) The training interface in the control condition, (b) mean number of time steps with feedback received by agents in level 0, 1 and 2 in the game. Note: error bars represent standard error.}
%%\vspace{-5mm}
%\label{control&feedback}
%\end{figure}

\begin{figure}[htb]
\centering
%\begin{tabular}{c c}
\includegraphics[width=0.45\linewidth]{control} %& 
%\includegraphics[width=0.4\columnwidth,height=1.7in]{NumberOfTimeStepsWithFeedbackScoreLevel.pdf}\\
%(a)&(b)
%\end{tabular}
%\vspace{-4mm}
\caption{The training interface used in the control condition. }%(b) mean number of time steps with feedback received by agents in level 0, 1 and 2 in the game. Note: error bars represent standard error.}
%\vspace{-5mm}
\label{control}
\end{figure}

%\begin{figure*}[htb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%\begin{tabular}{c c c}
%\includegraphics[width=0.7\columnwidth,trim=0 0 0 0, clip]{AgeGenderDistribution}&
%\includegraphics[width=0.4\columnwidth,trim=0 0 0 0, clip]{NumberOfTimeStepsWithFeedback}&
%\includegraphics[width=0.65\columnwidth,trim=0 0 0 0, clip]{MeanOfflinePerformancePerEpisode} \\
%a&b&c
%\end{tabular}
%\vspace{-3mm}
%\caption{ a) Distribution of subjects across age ranges and genders, (F=Female, M=Male), b) mean number of time steps with feedback, with and without facial expression and competitiveness and c) mean offline performance per episode trained by one of the authors. Note: black bars in Figure b stand for standard error.}
%\vspace{-6mm}
%\label{setup&feedback&performance}
%\end{figure*}

%As shown in Figure \ref{control}, 
In the interface, each bar in the performance window above the game board indicates the agent's performance in one game chronologically from left to right. The agent's performance is the score achieved by the Mario agent in the game task. During training, the pink bar represents the score received so far for the current game, while the dark blue bars represent the performance of past games. When a game ends (i.e., Mario dies), the corresponding bar becomes dark blue and a new score received in the new game is visualized by a pink bar to its right. When the performance window is full, the window is cleared and new bars appear from the left. Trainers in this condition were told to use key presses to train the agent.
%Participants could give positive and negative feedback by pressing buttons on the keyboard to reward or punish the previous action of the agent. Their feedback can be strengthened by pressing the button multiple times. The reward value was always bounded between -4 and +4. \sw{Move this to the beginning of Section 5?}

 %\HH{The reward value was always bounded between -4 and +4.}

%\subsection{\HH{We need a new name for this that doesn't sound so vindictive...}Trick Facial Expression Condition}

%\vspace{-1mm}
\subsection{Facial Expression Condition}
%\vspace{-1mm}
The interface used in this condition is the same as in the control condition except that trainers were told to use both key presses and facial expressions to train the agent. We told them this because we wanted to investigate whether telling trainers to use facial expressions as a separate channel to train the agent would affect the trainer's training and agent's learning, compared to trainers in the control condition. We hypothesize that telling trainers to use facial expressions to train the agent will result in worse performing agents than agents trained by those being told to use only key-press feedback to train agents. This is because telling participants to use facial expressions as separate reward signal could induce %higher cognitive load thus 
distraction from giving high quality key press feedback. In addition, we hypothesize that because of more posed facial behaviors by trainers in this condition, the expressiveness of trainers' facial expression will be higher than those in the control condition. %\sw{Why?} %We hypothesized that the facial behaviour would be more posed and less spontaneous and could be more tiring to give in this condition.

%The interface used in this condition is the same as in control condition. The only difference is that trainers in this condition were told to use both key presses and facial expressions to train the agent. We told them this because we wanted to investigate whether trainers would behave differently, e.g., by exaggerating their facial expressions, if they were told that they have a separate channel to use facial expression to train the agent, compared to trainers in the control condition. We hypothised that the facial behaviour would be more posed and less spontaneous and could be more tiring to give in this condition. %\HH{We hypothesise that the facial behaviour will be more posed and less spontaneous and could be more tiring to give.} 
%Additionally, we hypothesize that compared to trainers in the control condition, the agents trained in facial expression condition will perform worse because less key press feedback is given to train the agents while more desperate facial expressions were performed by trainers. \HH{This second hypothesis is an obvious one. We need to make sure that the hypotheses actually contribute to some interesting and perhaps surprising findings rather than just very obvious and boring things.}

%\vspace{-1mm}


\subsection{Competitive Condition}%Close-competitive Condition}

In the interfaces used in the control and facial expression conditions, only the agent's own performance was shown to the human trainer. Li et al.\ \cite{li2014learning} showed that putting people in a socio-competitive situation could further motivate them to give more feedback and improve the agent's performance. \textcolor{blue}{In this study, we aim to verify this result as well as investigate how it plays out when the socio-competitive setting involves people who know each other and are training at the same time in the same room.} %In addition, we hypothesize that the expressiveness of trainers' facial expressions in the competitive condition would increase as a result of the agent's competitive feedback compared to those in the control condition. %and the effect of competition on trainers' facial expressions. %We hypothesize that people will be further motivated to improve the agent's performance if they are put in a competitive situation where they can compare the performance of their agents with that of others, especially those closely related people. \sw{This sounds like it's a new idea.  We need to say that a previous study showed that this works and that we wanted to verify it and see how it plays out when people who know each other are training at the same time in the same room.}
Therefore, in the competitive condition, we allow the agent to indicate the rank and score of the other members of the group, who are all training their own agents simultaneously in the same room, as described in the experimental setup in the previous section. The groups typically consist of family members or close friends, e.g., children and (grand) parents, %or grandparents
 brothers and sisters. %\HH{it would be better to actually provide numerical statistics here rather than broad statements - we should at least have the relationship between the adult and the child that they are signing on behalf of.}. 

%We hypothesize that the trainers in the competitive condition will express more and %\HH{You need to express this in more measurable terms.. . I recall you meant 'extreme' facial expressions...this is an interesting one that we could have added to the discussion with Hamdi this afternoon!} 
%extreme facial emotions %and give more key press feedback 
%than those in the control condition. % and the trained agents will perform better finally. 

%This condition is called close-competitive not only because the trainers are closely related, but also they sit and train their own agents in the same location at the same time. %unlike experiment in \cite{li2014leveraging, li2014learning} where trainers compete with each other remotely on the internet and most probably trained their agent at different times 
%\HH{is the submission anonymous? If so, we have to be careful not to draw attention to ourselves as authors of the previous work.}.
%

\begin{figure}[htb]
\centering
\includegraphics[width=0.55\linewidth]{leaderboard}
%\vspace{-2mm}
\caption{The training interface for the competitive condition.}
%\vspace{-4mm}
\label{leaderboard}
\end{figure}

To implement this condition, we added a leaderboard to the right of the interface used in the control and facial expression conditions, as shown in Figure \ref{leaderboard}. 
In the leaderboard, the first names, scores and ranks of all the group members currently training the agent are shown. When the trainer starts training for the first time, her agent's performance is initialized to 0 and ranked in the leaderboard. Whenever the trainer finishes a game (i.e. Mario dies), the new game score and rank are updated in the leaderboard. To create more movement up and down in the leaderboard, the last game score is always used. The trainer can directly check her score and rank in the leaderboard. Therefore, the trainer can keep track of both the agent's learning progress and the agent's performance relative to that of other members of her group.


%\vspace{-1mm}
\subsection{Competitive Facial Expression Condition}%Trick Facial Expression and Close-compe-titive Condition}
%\HH{I'm not sure whether to use the word 'close'... it implies that there should be another condition where strangers compete with each other. However, we did not have a setup for this... so perhaps socio-competitive in some sense, is more accurate. Also, I think you need to explain why the members of groups were close - it's actually more related to how NEMO was set up.}
The final condition is a combination of the facial expression and competitive conditions. Specifically, the interface is the same as in the competitive condition but, as in the facial expression condition, trainers were told to use both key presses and facial expressions to train agents.  As in other conditions, only key presses were actually used for agent learning. \textcolor{blue}{We hypothesize that the expressiveness of trainers' facial expressions in this condition will be %much 
higher than those in the %facial expression condition and 
competitive condition, since trainers in this condition were %both 
told to use facial expressions as additional channel to train the agent. %and informed competitive feedback by the agent.
}

%With this condition, we aim to investigate how trainers behave if they are told to use facial expressions to train the agent in a %\HH{close?} socio-competitive 
%competitive situation and whether trainers in this condition behave and train the agent differently from those in the facial expression and competitive conditions. We hypothesize that agents trained in this condition will ultimately perform worse %and receive less key press feedback 
%%and trainers will make more %\HH{extreme} desperate
%%extreme facial expressions 
%compared to those in the competitive condition. Furthermore, we hypothesize that, compared with agents trained in the facial expression condition, agents trained in this condition will perform better finally and receive more key press feedback.% and trainers will make more \sw{Why?} extreme facial expressions. 


%\vspace{-1mm}
\section{Experimental Results}
%\vspace{-1mm}
\label{sec:re}

In this section, we present and analyze our experimental results. Given the large sample size and game design and participants in our study span a wide range of ages and two genders, we consider three additional individual variables: age, gender and level, in addition to `competition' and `facial expression' the two tested independent variables. %Here `facial expression' means whether telling trainers to use facial expressions to train agents or not, %as in the facial expression condition and competitive facial expression condition, 
%while `competition' means whether a leaderboard was shown in the training interface. % as in the competitive condition and competitive facial expression condition. 
`Gender' and `age' mean all trainers were divided into females and males and three age groups: under 13, 13 to 30 and older than 30. %, which is also a measure of variance by telling what proportion of the variance in the dependent variable is attributable to the factor in question.
`Level' means the most frequent highest level in the game reached by the agent tested offline: level 0, level 1 and level 2 in the game design. 

However, some other factors such as the distribution of the trainer's skill levels across conditions, experience in gaming especially in Super Mario, environmental stress, the domain stochasticity, participant's cultural difference etc., may still affect the results in our study. Nonetheless, we believe that the large number of participants can compensate for these variabilities encountered while running studies in the non-laboratory setting.
In the results below, the $p$-values were computed with $n$-way ANOVA ($n$ is the number of factors) since we need to take multiple factors into account, unless indicated otherwise. ANOVA is a test for statistical significance between means by actually comparing the variance due to the between-groups variability with the within-group variability. %for each factor. %\HH{comparing variance between what? The variance of what? You need to be more precise here.}.  \HH{this sentence doesn't really explain anything... it's like saying 'g' is usually used for testing the state. What is eta and how is the calculated? where is 'n'? }
$\eta_{p}^{2}$ is the effect size which indicates %the proportion of variance in the dependent variable explained by the independent variable. 
the variance accounted for by a factor effect and that effect plus its associated error variance within an ANOVA study. Note that the suggested norms for $\eta_{p}^{2}$ are: small = 0.01; medium = 0.06; large = 0.14.  %\cite{cohen2013statistical}. 
%Apart from `competition' and `facial expression' two independent variables, we consider three additional individual variables: %$n=5$ factors: facial expression, competition, 
%age, gender and level. %Here `facial expression' means whether telling trainers to use facial expressions to train agents or not, %as in the facial expression condition and competitive facial expression condition, 
%%while `competition' means whether a leaderboard was shown in the training interface. % as in the competitive condition and competitive facial expression condition. 
%`Gender' and `age' mean all trainers were divided into females and males and three age groups: under 13, 13 to 30 and older than 30. %, which is also a measure of variance by telling what proportion of the variance in the dependent variable is attributable to the factor in question.
%`Level' means the most frequent highest level in the game reached by the agent tested offline.

%\HH{you mention the inclusion of a mode anova later but don't mention it here.... that's pretty confusing...}
%\sw{It's important that the figures be readable in black and white, but it's also nice if they are in color for people who print on a color printer or read the pdf on a computer.}

%\vspace{-3mm}
%\paragraph{Feedback Given}
%\subsection{Feedback Given}
%%\vspace{-1mm}
%
%\begin{wrapfigure}{r}{0.5\columnwidth}
%\vspace{-8mm}
%\begin{center}
%\includegraphics[width=0.45\columnwidth]{NumberOfTimeStepsWithFeedbackScoreLevel.pdf}
%\vspace{-4mm}
%\caption{Mean number of time steps with feedback received by agents in level 0, 1 and 2 in the game. Note: error bars represent standard error.} %with and without facial expression and competitiveness.}
%\label{feedback}
%\end{center}
%\vspace{-4mm}
%\end{wrapfigure}

%\begin{figure}{r}
%\vspace{-8mm}
%\begin{center}
%\includegraphics[width=0.7\columnwidth]{NumberOfTimeStepsWithFeedbackScoreLevel}
%%\vspace{-10mm}
%\caption{Mean number of time steps with feedback for score level 0, 1 and 2.} %with and without facial expression and competitiveness.}
%\label{feedback}
%\end{center}
%%\vspace{-8mm}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=0.45\columnwidth]{NumberOfTimeStepsWithFeedback}
%\vspace{-6mm}
%\caption{Mean number of time steps with feedback, with and without facial expression and competitiveness.}
%\vspace{-4mm}
%\label{feedback}
%\end{figure}

%\begin{figure}[htb]
%\centering
%%\begin{tabular}{c c}
%%\includegraphics[width=0.5\linewidth]{control} & 
%\includegraphics[width=0.4\columnwidth,height=1.7in]{NumberOfTimeStepsWithFeedbackScoreLevel.pdf}%\\
%%(a)&(b)
%%\end{tabular}
%%\vspace{-4mm}
%\caption{Mean number of time steps with feedback received by agents in level 0, 1 and 2 in the game. Note: error bars represent standard error.}
%%\vspace{-5mm}
%\label{feedback}
%\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{FeedbackFrequency}
%\vspace{-6mm}
\caption{Mean number of time steps with feedback per 200 time steps for all four conditions during the training process.}
%\vspace{-4mm}
\label{feedback}
\end{figure}

\begin{figure}[htb]
%\vspace{-2mm}
%\hspace{-6mm}
\centering
\begin{tabular}{c c c c}
\includegraphics[width=0.35\columnwidth]{DistributionFinalOfflinePerformControl.pdf}&
\includegraphics[width=0.35\columnwidth]{DistributionFinalOfflinePerformFE.pdf}& \\
\includegraphics[width=0.35\columnwidth]{DistributionFinalOfflinePerformCompetitive.pdf}&
\includegraphics[width=0.35\columnwidth]{DistributionFinalOfflinePerformCompetitiveFE.pdf}
%(a)&(b)&(c)&(d)\\
%\vspace{-2mm}
\end{tabular}
\caption{Distribution of final offline performance across the four conditions. FE=Facial Expression.}%: a) Control condition, b) FE condition, c) Competitive condition and d) Competitive FE condition (FE=Facial Expression). %\HH{Could it make sense to actually color the bars here according to the highlest level that was reached for a given bar? It might help in better expalaining the multi-level scoring}}
%\vspace{-2mm}
\label{HisFinalOfflinePerformance}
\end{figure}

\subsection{Feedback Given}

Figure \ref{feedback} shows how feedback was distributed per 200 time steps over the learning process for the four conditions. From Figure \ref{feedback} we can see that the number of time steps with feedback received by agents in the four conditions increased at the early training stage and decreased dramatically afterwards, which supports previous studies \cite{knox2012humans,li2013using} and our motivation for investigating methods of enabling agents to learn from the trainer's facial expressions. In addition, trainers in the competitive and competitive facial expression condition \textcolor{blue}{seem} to have a trend to give more feedback than those in the control and facial expression conditions before 1000 time steps. \textcolor{blue}{Moreover, subjects in the facial expression condition tend to give a similar number of keypress feedback compared to those in the control condition and subjects in the competitive facial expression condition tend to give a similar number of keypress feedback compared to those in the competitive condition, even though they were told to give feedback via both keypress and facial expression.}%In addition, the number of feedback received by agents in the competitive and competitive facial expression condition is more than that in the control and facial expression conditions before 1000 time steps, which shows that the agent's competitive feedback can increase the number of feedback given by the trainer. %at least at the early training stage.

%Trainers in both informative conditions gave feedback for much longer than those in the control condition. Most notably, trainers in the uncertainty-informative condition gave a strikingly large amount of feedback, even during later intervals, with a much slower fall-off than the other conditions.

%%Figure \ref{feedback} 
%Our results show that both  `facial expression' and `competition' increase the amount of feedback given via key presses, though not significantly.
%%telling subjects to use facial expressions as a separate channel for giving feedback to train agents increases the amount of feedback given via key presses, and an agent's competitive feedback can also increase the amount of feedback given by the trainers even at a much broader age range (from 6 to 72) than was considered in \cite{li2014learning}, though not significantly. 
%However, Figure \ref{control&feedback}b shows that the higher the level the agents reach, the less feedback they receive ($F(2,490) = 9.11, p = 0.0001, \eta_{p}^{2} = 0.036$). 

%the mean number of time steps with feedback with and without  `facial expression' and `competition'. %summarizes the mean and median number of time steps with feedback for each condition. %\HH{Doesn't one time step equates to a certain number of ticks which is independent of when actions of the agent are taken...?} 
%One time step equates to the execution of one action by the agent. The results show that, `facial expression' does not decrease the number of time step with feedback provided by trainers ($F[1,492]$ = $0.57$, $p$ = $0.45$, $\eta_{p}^{2}$ = $0.001$), %($F[1,492] = 0.57, p = 0.4504, \eta_{p}^{2} = 0.0012$), 
%and `competition' increased the number of time step with feedback provided by trainers ($F[1,492]$ = $1.37$, $p$ = $0.24$, $\eta_{p}^{2}$ = $0.003$). 
%($F[1,492] = 1.37, p = 0.2426, \eta_{p}^{2} = 0.0028$). %in terms of both mean and median, in the competitive condition, trainers gave more feedback than those in the control condition ($p = 0.075$). Similarly, in the competitive facial expression condition, trainers gave more feedback than those in the facial expression condition ($p = 0.2298$). However, trainers in the facial expression condition and competitive facial expression condition gave similar amounts of feedback to those in the control and competitive conditions respectively.% ($p = 0.2842$ and $p = 0.5439$ respectively). \sw{What do these p-values mean?  Was the null hypothesis here that they were the same or different?}

%Overall, our results show that telling the subjects to use facial expressions as a separate channel for giving feedback for training agents does not decrease the amount of feedback given via key presses, and an agent's competitive feedback can increase the amount of feedback given by the trainers even at a much broader age range (from 6 to 72) than was considered in \cite{li2014learning}, though not significantly.

%provide additional evidence that, even at a much broader age range (from 6 to 72) than was considered in \cite{li2014leveraging}, an agent's competitive feedback can increase the amount of feedback given by the trainers. Moreover, telling the subjects to use facial expressions as a separate channel for giving feedback for training agents does not decrease the amount of feedback given via key presses.





%\vspace{-3mm}
%\paragraph{Performance}
\subsection{Performance}
\label{sec:per}

%\begin{figure}[tb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%\includegraphics[width=0.7\columnwidth,trim=0 0 0 0, clip]{MeanOfflinePerformancePerEpisode} 
%\vspace{-2mm}
%\caption{Mean offline performance per episode trained by one of the authors.}
%\vspace{-4mm}
%\label{MeanOfflinePerformanceEpisode}
%\end{figure}

%First, to see whether a TAMER agent can successfully learn to play the game and compare the learning performance with other methods, one of the authors trained the agent on difficulty level 0 with seed 121 \HH{I dont' recall any prior mention of the difficulty level or seed. If this is important, then you should also mention this in the experimental set up.} in the Infinite Mario Domain for 10 trials with TAMER. %Figure \ref{setup&feedback&performance}c shows the average offline performance per episode. 
%We take a dying of Mario and finishing the level as one episode. The policy was frozen and recorded at the end of each episode of training. Then, each policy was tested for 20 games offline. The performance for each episode %in Figure \ref{setup&feedback&performance}c 
%was averaged over the 20 games and then over the 10 trials. The result shows that our TAMER agent can achieve 120 points in 12 episodes, while it takes about 500 episodes for a SARSA agent to achieve a similar performance level \cite{taylor2011teaching} and a hierarchical SARSA agent implemented with object-oriented representation 300 episodes to achieve 149.48 points \cite{mohan2011object}, which is almost optimal. Although the TAMER agent does not learn an optimal policy, it can successfully learn a good policy substantially faster than these other methods, making this set-up very suitable for our experiments with members of the public where each training session can only last up to 15 minutes. %\HH{It's great that you did this. Very good!} 
%\HH{I wonder if this part would be better described in the experimental set up. This part isn't really a result in itself but a way of showing that we did some serious experimental preparation to verify the efficacy of the set up. }


%
%To see whether the agent's competitive feedback can motivate a human trainer to give better feedback and ultimately improve the agent's learning, we examined how the agent's performance varied as its policy changed over time in the four conditions. We divided the whole training process into intervals, with each interval composed of 200 time steps. As before, the agent's policy was frozen and tested offline for 20 games and performance was averaged across all subjects within each condition. However, some factors such as the distribution of the trainer's skill level across conditions, the domain stochasticity etc., may still affect the evaluation of agent performance. Nonetheless, we believe that the large number of participants in our study can partially compensate for these variabilities. 
%
%Our results show that an agent's competitive feedback can motivate human trainers to train agents better regardless of whether they are told to use facial expressions as a separate channel to train the agent or not. As shown in Figure \ref{finalofflineperformance}, agents in the competitive condition ultimately outperform those in the control condition, especially in terms of the median ($p = 0.2952$). Similarly, agents in the competitive facial expression condition ultimately outperform those in the facial expression condition ($p = 0.2096$). Moreover, agents in the facial expression condition and competitive facial expression condition ultimately perform worse than those in the control and competitive condition respectively ($p = 0.2072$ and $p = 0.2827$). %\sw{Discuss why the mean and median are so different.}
%
%In summary, our results verify that the agent's competitive feedback can improve its performance. In addition, the results are consistent with our hypothesis that telling the subject to use facial expressions as a separate channel to give feedback has a negative effect on the agent's learning.
%
%However, the mean and median performance are quite different and the differences between conditions are not statistically significant. 

%see whether the agent's competitive feedback and telling to use facial expressions as separate channel have effect on agent learning and 
To get some exploratory insights into our data, we first analyzed the distribution of final offline performance for each condition with the learned final policy tested offline for 20 games and averaged over the 20 games for each subject. %as shown in Figure \ref{HisFinalOfflinePerformance}. 
Figure \ref{HisFinalOfflinePerformance}, which contains histograms of the final offline performance for the four conditions, shows that the distribution in the four conditions are all characterized by three modes. The gap between modes is caused by the score mechanism which gives credit +100 for finishing one level and much less otherwise. Therefore, the three modes in Figure \ref{HisFinalOfflinePerformance} from left to right correspond to level 0, 1 and 2 in the game respectively.

%\HH{It's really strange to say this and not give any explanation of why! By the way, the following sentence provides an observation but not explanation for this result. Why dont' you include something about the level number that was reached for each mode?}.  
%In each condition, many subjects' agents performed poorly, with scores below 50, reflecting a failure to master basic skills.  A moderate number of subjects' agents performed solidly, with scores around 125, on par with the agent we trained. %(see Figure \ref{setup&feedback&performance}c).  
%A small number of subjects' agents performed exceptionally, with scores in excess of 250.  


%\begin{figure*}[htb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%%\begin{tabular}{c c c}
%\includegraphics[width=0.75\columnwidth,trim=0 0 0 0, clip]{CompetitionPerMode}&
%\includegraphics[width=0.45\columnwidth,trim=0 0 0 0, clip]{FacialExpressionPerGender}&
%%\includegraphics[width=0.5\columnwidth,trim=0 0 0 0, clip]{CompetitionPerGender}&
%\includegraphics[width=0.60\columnwidth,trim=0 0 0 0, clip]{CompetitionPerAge} \\
%%\vspace{-2mm}
%(a)&(b)&(c)
%\end{tabular}
%\vspace{-4mm}
%\caption{Mean final offline performance for a) agents across and within mode 1, 2, 3 distributed in Figure \ref{HisFinalOfflinePerformance} from left to right trained with and without competitiveness, b) agents trained by female and male subjects with and without facial expression, and c) agents trained by trainers in three age groups with and without competitiveness. Note: black bars stand for standard error. %and mode 1 corresponding to the left mode (level 0), mode 2 corresponding to the middle mode (level 1) and mode 3 corresponding to the right mode (level 2) in a).
%}
%\vspace{-5mm}
%\label{performance}
%\end{figure*}

%The histograms highlight a similar distribution in both competitive conditions and between both non-competitive conditions.
%% the competitive and competitive facial expression conditions, and in conditions. %also show that the distribution of final offline performance in the competitive condition and competitive facial expression condition are similar, while the distributions of final offline performance in the control and facial expression condition are also similar. 
%Although the number of subjects who trained the agent well 
%%\HH{ be careful with the use of a word that implies time dependence...}
%% increased 
%was higher (increased by 32)
%because of the competitive element in the corresponding condition, 
%% competitiveness in the competitive and competitive facial expression conditions, 
%the number of subjects who trained agents poorly was 
%% high or 
%%\HH{ be careful with the use of a word that implies time dependence...}
%% also increased. 
%similarly high (increased by 49). %\HH{ if you want to talk about similar or higher values, you really need to provide numerical evidence. These statements about the quantative are currently too qualitative...}



%This may partially explain the large difference between mean and median performance and the lack of significance in the performance differences between conditions.
%\HH{Nice!}

%Given there are three modes in the distribution of final offline performance and a broad range of ages and genders across trainers, to get a deep insight of the data, we consider mode of the final offline performance as an additional factor in the n-way ANOVA test.% in terms of five factors: , 
%facial expression, competition, age and gender. Here `facial expression' means whether telling trainers to use facial expression to train the agent or not, %as in the facial expression condition and competitive facial expression condition, 
%while `competition' means whether there is leaderboard in the training interface.% as in the competitive condition and competitive facial expression condition. 
%`Gender' and `age' mean all trainers were divided into females and males and three age groups: under 13, 13 to 30 and older than 30 years old. 

%\begin{table}[htb]
%%\renewcommand{\arraystretch}{1.5}
%%\captionsetup{font=footnotesize}
%
%\centering
%\begin{tabular}{ r | l | l | l }
%\toprule
%%\multirow{2}{*}{Condition} &Non-social  &  Social behavior \\
%& \textbf{Mode 1} & \textbf{Mode 2}  & \textbf{Mode 3}\\
%\toprule
%\textbf{Competition} & $p = 0.1271$ & $p$ = $\textbf{0.0334}^{\ast}$ & $p$ = $\textbf{0.0135}^{\ast}$ \\
%\hline
%\textbf{Gender} & $p = 0.3777$ & $p = 0.6141$ &  $p = 0.6055$ \\
%\hline
%\textbf{Age} & $p = 0.4726$ & $p = 0.4032$ & $p = 0.2069$\\
%\hline
%\textbf{FE} & $p = 0.5098$ & $p = 0.8976$ & $p = 0.2482$ \\
%\toprule
%\end{tabular}
%\caption{ANOVA test of factor effect on final agent performance for the three performance modes shown in Figure \ref{HisFinalOfflinePerformance}. Note: mode 1 corresponding to the left mode, mode 2 corresponding to the middle mode and mode 3 corresponding to the right mode in Figure \ref{HisFinalOfflinePerformance}. $^{\ast}$Statistically significant.}
%%\vspace{-4mm}
%\label{mode}
%\end{table}

\begin{figure}[htb]
\vspace{4mm}
%\hspace{4mm}
\centering
%\begin{tabular}{c c c}
\includegraphics[width=0.5\columnwidth, trim=0 0 0 0, clip]{CompetitionPerMode}%&
%\includegraphics[width=0.38\columnwidth, height=1.56in, trim=0 0 0 0, clip]{FacialExpressionPerGender}&
%\includegraphics[width=0.50\columnwidth, height=1.56in, trim=0 0 0 0, clip]{CompetitionPerAge} \\
%(a)&(b)&(c)
%\end{tabular}
%\vspace{2mm}
\caption{Mean final offline performance for agents across and in the three modes distributed in Figure \ref{HisFinalOfflinePerformance}, trained with and without competitiveness. % trained by (b) female and male subjects with and without facial expression, (c) trainers in three age groups with and without competitiveness. 
Note: black bars stand for standard error.}
%\vspace{4mm}
\label{Mode}
\end{figure}


%Given there are three modes in the distribution of final offline performance with a large difference in performance, %and a broad range of ages and genders across trainers, 
%to get a deep insight of the data, 
%we consider the mode of the final offline performance as an additional factor in the $n$-way ANOVA test.  
%\HH{how can you consider a mode as an additional factor? This doesn't make sense. You probably have to assign an integer to label a particular mode. However, you need to do this systematically. How was this determined? Why aren't you talking about the highest level that mario reached? The way you describe it now seems like you numbered the start and end of each node manually yourself... which is not what I had discussed with you before....You remember after my discussion with the psychologists in Eindhoven that you should consider splitting the modes based on the maximum level surpassed?I'm surprised that you have not written this anyway. It makes me rather concerned that you present here the experiments from the analysis that you already did before the submission of last year's aamas submission...}. 
Figure \ref{Mode} highlights the importance of `competition'. %for all agents and agents distributed in the three modes in Figure \ref{HisFinalOfflinePerformance}. %When taking all five factors together, %the results show that %the mode of the final offline performance has a significant role on agent learning performance ($p = 0$), which is obviously and clearly shown in Figure \ref{HisFinalOfflinePerformance}, and 
\textcolor{blue}{When taking all five factors together, i.e., `competition', `facial expression', `age', `gender' and `level', we did a 5-way ANOVA to investigate their main effects and interactions. %For all agents across all modes, Figure \ref{Mode} shows that 
The 5-way ANOVA results show that `competition' can motivate trainers to train agents significantly better ($F(1,471)$ = $4.45$, $p$ = $0.035$, $\eta_{p}^{2}$ = $0.009$), as shown in Figure \ref{Mode}. %($F[1,490] = 5.33, p = 0.0214, \eta_{p}^{2} = 0.0108$). %while `age' and `gender' factors have insignificant effect on agent final performance ($F[2,490] = 1.47, p = 0.2307, \eta_{p}^{2} = 0.006$ and $F[1,490] = 0.98, p = 0.3223, \eta_{p}^{2} = 0.002$, respectively). 
Moreover, when looking into the interaction between the five factors, we found that the interaction between `facial expression' and `gender' has a significant %negative 
effect %on agent performance 
($F[1,471]$ = $6.49$, $p$ = $0.011$, $\eta_{p}^{2}$ = $0.014$). }%($F[1,471] = 5.06, p = 0.0249, \eta_{p}^{2} = 0.0106$). 

\textcolor{blue}{To examine how `competition' and `facial expression` affect trainers and agent performance in different modes, gender and age groups, 4-way ANOVA test was conducted by taking the remaining four factors into account. First, %While examining %the factor effect for each mode, i.e., 
we take `competition', `facial expression', `age' and `gender'  into account and did a 4-way ANOVA for agents in each mode in Figure \ref{HisFinalOfflinePerformance} respectively. %as shown in Table \ref{mode}, 
Our 4-way ANOVA results show %Figure \ref{Mode} shows 
that `competition' can significantly improve the agent's learning in %mode 2 ($F[1,116]$ = $4.64$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.015$) and 
mode 3 ($F(1,19)$ = $7.41$, $p$ = $0.01$, $\eta_{p}^{2}$ = $0.04$),  %($F[1,116] = 4.64, p = 0.0334, \eta_{p}^{2} = 0.0152$) and mode 3 ($F[1,19] = 7.41, p = 0.0135, \eta_{p}^{2} = 0.0399$) 
%respectively, %corresponding to the middle and right modes in Figure \ref{HisFinalOfflinePerformance} 
in which agents performed best (level 2 in the game). Moreover, `gender' and the interaction between `gender' and `facial expression' have significant effects ($F[1,332]$ = $4.07$, $p$ = $0.045$, $\eta_{p}^{2}$ = $0.012$ and $F[1,332]$ = $6.75$, $p$ = $0.0098$, $\eta_{p}^{2}$ = $0.020$ respectively) for agents in mode 1 (level 0 in the game). }
%However, when examining `facial expression', there is no effect on agents' learning across modes or within each mode.} 
\textcolor{blue}{Then we take `competition', `facial expression', `gender' and `level' into account and did a 4-way ANOVA for agents trained by subjects in the three age groups respectively. Our 4-way ANOVA results show that the interaction between `gender' and `facial expression' have a significant effect for agents trained by subjects below 13 years old ($F[1,232]$ = $7.1$, $p$ = $0.008$, $\eta_{p}^{2}$ = $0.030$). Moreover, subjects above 30 years old affected by `competition' tend to train agents better ($F(1,114)$ = $3.05$, $p$ = $0.083$, $\eta_{p}^{2}$ = $0.026$).
Lastly, we take `competition', `facial expression', `age' and `level' into account and did a 4-way ANOVA for agents trained by female and male subjects respectively. Our 4-way ANOVA results show that male subjects affected by `competition' tend to train agents better ($F(1,286)$ = $3.41$, $p$ = $0.066$, $\eta_{p}^{2}$ = $0.012$).
%When looking into each mode in Figure \ref{HisFinalOfflinePerformance}, we found that within mode 1 (level 0 in the game), `facial expression' ruins agent learning trained by female subjects ($F(1,332)$ = $6.75$, $p$ = $0.01$, $\eta_{p}^{2}$ = $0.02$). However, we found no significant gender-specific effect of `competition' on training. Moreover, while examining the `age', we found that `competition' can significantly motivate trainers more than 30 years old to train the agent better ($F(1,123)$ = $4.65$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.036$) but not those less than 30.
} 



%\begin{figure}[htb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%%\begin{tabular}{c c}
%\includegraphics[width=0.75\columnwidth,trim=0 0 0 0, clip]{CompetitionPerMode}
%%\includegraphics[width=0.495\columnwidth,trim=0 0 0 0, clip]{MedianOfflinePerformance} \\
%%a&b
%%\end{tabular}
%\vspace{-4mm}
%\caption{Mean final offline performance for agents across and in the three modes distributed in Figure \ref{HisFinalOfflinePerformance}, trained with and without competitiveness. Note: black bars stand for standard error.}%mode 1 corresponding to the left mode (level 0), mode 2 corresponding to the middle mode (level 1) and mode 3 corresponding to the right mode (level 2) in Figure \ref{HisFinalOfflinePerformance}. }
%\vspace{-4mm}
%\label{mode}
%\end{figure}

%especially while trained by good trainers. 
%However, though the results are significant, differences between the mean performances are not large. We believe that it could be because of the inappropriate reward for easy and difficult agent's behaviors by the score mechanism.%who can train agents to perform better. %While our results also show that the interaction of `facial expression' and `gender' has a significant negative effect on agent performance, we are curious to know how `facial expressions' and `competition' affect different genders and age groups.

%In addition, we found that there is a two-way interaction between `facial expression' and `gender':  `facial expression' has a significantly negative effect on agent learning trained by female trainers ($F(1,185)$ = $4.57$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.024$) but not by male ones. 
%When looking into each mode in Figure \ref{HisFinalOfflinePerformance}, we found that within mode 1 (level 0 in the game), %agents trained by female trainers performed significantly better than by males ones %a significant effect on agent's learning performance was caused by `gender' 
%%($F(1,332)$ = $4.07$, $p$ = $0.045$, $\eta_{p}^{2}$ = $0.012$) and %there is a two-way interaction between `facial expression' and `gender': 
%`facial expression' ruins agent learning trained by female subjects ($F(1,332)$ = $6.75$, $p$ = $0.01$, $\eta_{p}^{2}$ = $0.02$). However, we found no significant gender-specific effect of `competition' on training.

%Moreover, while examining the `age', we found that `competition' can significantly motivate trainers more than 30 years old to train the agent better ($F(1,123)$ = $4.65$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.036$)  %not trainers younger than 30 years old. %under 13 years old ($F[1,241]$ = $2.99$, $p$ = $0.085$, $\eta_{p}^{2}$ = $0.012$), %($F[1,241] = 2.99, p = 0.0848, \eta_{p}^{2} = 0.0123$), 
%though not significant. 
%but not %has no effect on trainers 
%those less than 30. %While examining `facial expression'
%Furthermore, we found that %it has some negative effect on agent training by subjects older than 30 years old ($F(1,123)$ = $2.42$, $p$ = $0.12$, $\eta_{p}^{2}$ = $0.019$), %($F[1,123] = 2.42, p = 0.1225, \eta_{p}^{2} = 0.0193$), 
%and %
%there is a two-way interaction between `facial expression' and `gender' for trainers less than 13 years old: `facial expression'  ruins agent learning trained by female participants less than 13 ($F(1,232)$ = $7.1$, $p$ = $0.008$, $\eta_{p}^{2}$ = $0.03$).


\textcolor{blue}{Overall, these results support prior results \cite{li2014learning} demonstrating the importance of bi-directional interaction and competitive elements in the training interface, and show that `competition' can significantly improve agent learning and help the best trainers the most. Specifically, `competition' can motivate male subjects and those above 30 years old to train agents better. Moreover, our results show that the interaction of `facial expression' and `gender' has significant effect for subjects who can only train agents to reach level 0 most frequently and those are less than 13 years old. %our results show the role of age and gender in trainer behavior and agent performance affected by `facial expression' and `competition'.
}

%Moreover, our results show that %`facial expression' -- telling subjects to use facial expressions as a separate training channel has some effect on final agent performance trained by subjects older than 30 years old and
%`competition' can significantly motivate participants more than 30 to train agents better not those who are younger.
%In addition, our results suggest that `facial expression' %telling subjects to use facial expressions as a separate training channel 
%has a significantly negative effect on agent training by female participants, especially those who are less than 13 years old and cannot train agents to perform well. %that can only reach level 0 most frequently, % not by male trainers, 
%and `competition' has no effect on agent training by males and females respectively. 
%Moreover, our results show that %`facial expression' -- telling subjects to use facial expressions as a separate training channel has some effect on final agent performance trained by subjects older than 30 years old and
%`competition' can significantly motivate subjects older than 30 years old to train agent better not those younger ones. %than 30 years old. 
%Furthermore, %for trainers between 13 and 30 years old, agents trained by male subjects performed better than those by female ones and %the two-way interaction effect was on trainers younger than 13 years old.
% %telling to use facial expression as a separate training channel 
%`facial expression' has a negative effect on agent learning trained by female subjects younger than 13 years old.

%\begin{table}
%%\renewcommand{\arraystretch}{1.5}
%%\captionsetup{font=footnotesize}
%\centering
%\begin{tabular}{ r | l | l }
%\toprule
%%\multirow{2}{*}{Condition} &Non-social  &  Social behavior \\
%& \textbf{Female} & \textbf{Male}  \\
%\toprule
%\textbf{Perfor. mode} & $p$ = $\textbf{0}^{\ast}$ & $p$ = $\textbf{0}^{\ast}$ \\
%\hline
%\textbf{Competition} & $p = 0.4881$ & $p$ = $\textbf{0.007}^{\ast}$ \\
%\hline
%\textbf{Age} & $p$ = \textbf{0.076} & $p = 0.9655$\\
%\hline
%\textbf{FE} & $p$ = $\textbf{0.0339}^{\ast}$ & $p = 0.2039$ \\
%\toprule
%\end{tabular}
%\caption{ANOVA test of factor effect on final agent performance for female and male subjects. $^{\ast}$Statistically significant.}
%%\vspace{-4mm}
%\label{gender}
%\end{table}

%\sw{We also need something here to support our claim that the results show TAMER can be used to learn Mario.  How do these average scores compare to RL benchmarks after the same number of episodes?  What kind of qualitative performance do they correspond to?  Can we argue that Mario was able to master basic skills due to TAMER training?}

%\HH{What happened to the learning curves? Can we include some of them? The dip in learning was something that could be discussed.. and how it varies across gender/age...?}


%\vspace{-3mm}
%\paragraph{Role of Gender}
%\subsection{Role of Gender}
%\begin{figure*}[htb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%\begin{tabular}{c c}
%\includegraphics[width=0.8\columnwidth,trim=0 6 10 10, clip]{FinalOfflinePerformanceForFemale} &
%\includegraphics[width=0.8\columnwidth,trim=0 6 10 10, clip]{FinalOfflinePerformanceForMale}\\
%a&b
%\end{tabular}
%\caption{Final offline performance for female (a) and male (b) subjects. (FE=Facial Expression.)}
%\label{FinalOfflinePerformanceGender}
%\end{figure*}



%While our results show that competitive feedback can significantly improve agent final performance, %that the interaction of `facial expression' and `gender'  has significantly negative effect on the agent performance. %motivate human trainers to give better feedback and improve the agent's ultimate performance.  
%%telling trainers to use facial expressions as a separate channel to give feedback 
% %has a negative effect on the agent's learning. 
%%Moreover, 
%%When looking into the interaction between the five factors, we found that the interaction of `facial expression' and `gender' has a significantly negative effect on agent performance ($F[1,471]$ = $5.06$, $p$ = $0.02$, $\eta_{p}^{2}$ = $0.011$).
%we wanted to know which gender `competition' and `facial expression' affect more, since females and males often perceive and react to social situations differently, e.g., girls have been shown to be significantly more relationally aggressive than boys \cite{crick1995relational}.
%%Moreover,

%We found that %there is a two-way interaction between `facial expression' and `gender':  
%`facial expression' has a significantly negative effect on agent learning trained by female trainers ($F(1,185)$ = $4.57$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.024$) but not by male ones, as shown in Figure \ref{ModeGengerAge}b.
%%($F(1,471)$ = $6.49$, $p$ = $0.01$, $\eta_{p}^{2}$ = $0.014$). 
%When looking into each mode in Figure \ref{HisFinalOfflinePerformance}, we found that within mode 1 (level 0 in the game), %agents trained by female trainers performed significantly better than by males ones %a significant effect on agent's learning performance was caused by `gender' 
%%($F(1,332)$ = $4.07$, $p$ = $0.045$, $\eta_{p}^{2}$ = $0.012$) and %there is a two-way interaction between `facial expression' and `gender': 
%`facial expression' ruins agent learning trained by female subjects ($F(1,332)$ = $6.75$, $p$ = $0.01$, $\eta_{p}^{2}$ = $0.02$). % in mode 1 (level 0 in the game).
%Moreover, we found no significant gender-specific effect of `competition' on training.


%Therefore, we want to know which gender `facial expression' and `competition' affect more, since females and males often perceive and react to social situations differently, e.g., girls have been shown to be significantly more relationally aggressive than boys \cite{crick1995relational}. %We hypothesize that the effect of competitive feedback would be different for male and female subjects, since they often perceive and react to social situations differently, e.g., girls have been shown to be significantly more relationally aggressive than boys \cite{crick1995relational}.  %\HH{removing this citation. i don't see the link..venkatesh2000don} %\sw{Need supporting cites here}. 
%Therefore, we analyzed the offline performance for male and female subjects across the four conditions.
%
%Therefore, we did n-way ANOVA test with the other four factors on agent final performance for female and male subjects separately.% as shown in Table \ref{gender}.
%
%From Table \ref{gender} we can see that, `performance mode' still has significant effects on final agent performance for both females and males ($p = 0$). Recall that results in Section \ref{sec:per} show that the interaction of `facial expression' and `gender' has a significant effect on agent performance, Table \ref{gender} 


% In summary, our results suggest that `facial expression' %telling subjects to use facial expressions as a separate training channel 
% has a significantly negative effect on agent training by female subjects, especially those trained agents insufficiently well, %that can only reach level 0 most frequently, % not by male trainers, 
% and `competition' has no effect on agent training by males and females respectively. %some effect on agent learning trained by male subjects. %can significantly improve agent learning trained by male subjects. %not by female subjects. 
%Interestingly, similar gender differences were also observed by other researchers \cite{brody1993understanding}, where females express a wider variety of emotions than do males, both verbally and through facial expressions. This could relate to our results about the significant effect of `facial expression' on agent performance trained by female subjects, though more investigation needs to be made into whether female trainers made more facial expressions than male trainers in our experimental setting.

%%\vspace{-4mm}
%\begin{figure}[hb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%\begin{tabular}{c c}
%%\includegraphics[width=0.58\columnwidth, height=1.56in, trim=0 0 0 0, clip]{CompetitionPerMode}&
%\includegraphics[width=0.38\columnwidth, height=1.56in, trim=0 0 0 0, clip]{FacialExpressionPerGender}&
%\includegraphics[width=0.50\columnwidth, height=1.56in, trim=0 0 0 0, clip]{CompetitionPerAge} \\
%(a)&(b)
%\end{tabular}
%%\vspace{-4mm}
%\caption{Mean final offline performance for agents %(a) across and in the three modes distributed in Figure \ref{HisFinalOfflinePerformance}, trained with and without competitiveness, 
%trained by (a) female and male subjects with and without facial expression, (b) trainers in three age groups with and without competitiveness. Note: black bars stand for standard error.}
%%\vspace{-5mm}
%\label{GengerAge}
%\end{figure}

%\paragraph{Role of Age}
%\subsection{Role of Age}
%\begin{figure}[tb]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%%\begin{tabular}{c c}
%\includegraphics[width=\columnwidth,trim=0 6 6 6, clip]{CompetitionPerAge}
%%\includegraphics[width=0.495\columnwidth,trim=0 0 0 0, clip]{MedianOfflinePerformance} \\
%%a&b
%%\end{tabular}
%%\vspace{-2mm}
%\caption{Mean final offline performance for agents in three age groups, with and without competitiveness. Note: black bars stand for standard error. }
%%\vspace{-8mm}
%\label{mode}
%\end{figure}

%\begin{table}
%%\renewcommand{\arraystretch}{1.5}
%%\captionsetup{font=footnotesize}
%\centering
%\begin{tabular}{ r | l | l | l }
%\toprule
%%\multirow{2}{*}{Condition} &Non-social  &  Social behavior \\
%& \textbf{Under 13} & \textbf{13 to 30}  & \textbf{Above 30}\\
%\toprule
%\textbf{Perfor. mode} & $p$ = $\textbf{0}^{\ast}$ & $p$ = $\textbf{0}^{\ast}$ &  $p$ = $\textbf{0}^{\ast}$ \\
%\hline
%\textbf{Competition} & $p$ = \textbf{0.0848} & $p = 0.6189$ & $p$ = $\textbf{0.033}^{\ast}$ \\
%\hline
%\textbf{Gender} & $p = 0.7827$ & $p$ = \textbf{0.0967} & $p = 0.6443$\\
%\hline
%\textbf{FE} & $p = 0.6634$ & $p = 0.9976$ & $p = 0.1225$ \\
%\toprule
%\end{tabular}
%\caption{ANOVA test of factor effect on final agent performance for trainers under 13 years old, 13 to 30 years old and older than 30 years old. $^{\ast}$Statistically significant.}
%%\vspace{-4mm}
%\label{age}
%\end{table}

%%Although Section \ref{sec:per} showed that overall, `age' has insignificant effect on agent's final offline performance, 
%We also suspected that `facial expression' and `competition' affect agent training by trainers within different agent groups differently, %the agent's competitive feedback and telling subjects to use facial expressions as a separate training channel have different effects on subjects of different ages, 
%since age differences largely account for reaction time differences \cite{deary2005reaction} and both brain size and cognitive ability increase until young adulthood and then decrease \cite{rushton1996brain}. %Moreover, our results show that `age' has some effect on agent final performance trained by female subjects ($F(1,185)$ = $2.61$, $p$ = $0.076$, $\eta_{p}^{2}$ = $0.027$). %\sw{This is vague.  How are they different and why? Support with citations}\HH{I would talk about 'cognitive' ability. Maybe you can find papers about sensemaking ability as a function of age.}. 
%
%To investigate the role of age, we did n-way ANOVA test with the other four factors on agent final performance for trainers within the three age groups separately.% as shown in Table \ref{age}.%divided the subjects into three age groups: under 13, between 13 and 30, and over 30 years old. Then we analyzed the final offline performance for the three age groups.
%
%\begin{figure*}[t]
%%\vspace{-2mm}
%%\hspace{4mm}
%\centering
%\begin{tabular}{c c c}
%\includegraphics[width=0.6\columnwidth,trim=0 6 10 10, clip]{FinalOfflinePerformanceForUnder13} &
%\includegraphics[width=0.6\columnwidth,trim=0 6 10 10, clip]{FinalOfflinePerformanceFor1330}&
%\includegraphics[width=0.6\columnwidth,trim=0 6 10 10, clip]{FinalOfflinePerformanceForOlder30}\\
%a&b&c
%\end{tabular}
%\vspace{-2mm}
%\caption{Final offline performance for subjects under 13 (a), between 13 and 30 (b) and older than 30 years old (c). (FE=Facial Expression.)}
%\vspace{-3mm}
%\label{FinalOfflinePerformanceAge}
%\end{figure*}

%%From Table \ref{age} we can see that, `performance mode' still has significant effects on final agent performance for trainers within the three age groups: under 13, 13 to 30 and older than 30 years old ($p = 0$). 
%%Moreover, for trainers under 13 years old, `facial expression' has no effect on final agent performance ($F[1,241] = 0.19, p = 0.6634, \eta_{p}^{2} = 0.0008$) and there is no difference between agent training by female and male subjects ($F[1,241] = 0.08, p = 0.7827, \eta_{p}^{2} = 0.0003$), but 
%Our results show that `age' has some effect on agent performance trained by female subjects with those between 13 to 30 years old training agents the worst ($F(1,185)$ = $2.61$, $p$ = $0.076$, $\eta_{p}^{2}$ = $0.027$).
%Moreover, Figure \ref{ModeGengerAge}c shows that `competition' can significantly motivate trainers older than 30 years old to train the agent better ($F(1,123)$ = $4.65$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.036$)  %not trainers younger than 30 years old. %under 13 years old ($F[1,241]$ = $2.99$, $p$ = $0.085$, $\eta_{p}^{2}$ = $0.012$), %($F[1,241] = 2.99, p = 0.0848, \eta_{p}^{2} = 0.0123$), 
%%though not significant. 
%but not %has no effect on trainers 
%those younger than 30 years old. Furthermore, for trainers between 13 to 30 years old, males trained better than females ($F(1,116)$ = $5.75$, $p$ = $0.02$, $\eta_{p}^{2}$ = $0.047$).
%%agents trained by male subjects performed better than those by female ones %`competition' have no effect on agent performance %$F[1,116] = 0, p = 0.9976, \eta_{p}^{2} = 0$ and 
%%($F(1,116)$ = $5.75$, $p$ = $0.02$, $\eta_{p}^{2}$ = $0.047$). %($F[1,116] = 0.25, p = 0.6189, \eta_{p}^{2} = 0.0021$), 
%%but agents trained by male subjects performed better than those by female ones %there is almost significant difference between agent performance trained by female and male subjects 
%%($F[1,116]$ = $2.8$, $p$ = $0.098$, $\eta_{p}^{2}$ = $0.024$). %($F[1,116] = 2.8, p = 0.0967, \eta_{p}^{2} = 0.0236$). 
%%For trainers older than 30 years old, %`facial expression' has some effect on final agent performance though not significant ($F[1,123] = 2.42, p = 0.1225, \eta_{p}^{2} = 0.0193$) %and there is no significant difference between agent performance trained by female and male subjects ($F[1,123] = 0.21, p = 0.6443, \eta_{p}^{2} = 0.0017$), but 
%%`competition' can significantly motivate trainers to train agent better ($F[1,123]$ = $4.65$, $p$ = $0.03$, $\eta_{p}^{2}$ = $0.036$). %($F[1,123] = 4.65, p = 0.033, \eta_{p}^{2} = 0.0364$). 
%While examining `facial expression', we found that it %has some negative effect on agent training by subjects older than 30 years old ($F(1,123)$ = $2.42$, $p$ = $0.12$, $\eta_{p}^{2}$ = $0.019$), %($F[1,123] = 2.42, p = 0.1225, \eta_{p}^{2} = 0.0193$), 
%%and %there is a two-way interaction between `facial expression' and `gender' for trainers younger than 13 years old: `facial expression' 
%ruins agent learning trained by female subjects younger than 13 years old ($F(1,232)$ = $7.1$, $p$ = $0.008$, $\eta_{p}^{2}$ = $0.03$).%but no effect by trainers under 30 years old. 

% In summary, our results suggest that %`facial expression' -- telling subjects to use facial expressions as a separate training channel has some effect on final agent performance trained by subjects older than 30 years old and
%  `competition' can significantly motivate subjects older than 30 years old to train agent better not those younger ones. %than 30 years old. 
% Moreover, %for trainers between 13 and 30 years old, agents trained by male subjects performed better than those by female ones and %the two-way interaction effect was on trainers younger than 13 years old.
% %telling to use facial expression as a separate training channel 
% `facial expression' has a negative effect on agent learning trained by female subjects younger than 13 years old. %, but has some negative effect on agent learning by subjects younger than 13 years old. %In addition, our results also show that there is almost significant difference between final agent performance trained by female and male subjects between 13 and 30 years old. 
%
%Since the brain size and cognitive ability show a curvilinear relation with age: increasing to young adulthood and then decreasing \cite{rushton1996brain}, 
%
%These results provide a deep insight on human training the agent especially when we allow agents to learn from facial expressions, though not significant. 

%\vspace{1mm}
%\section{Discussion}
%\label{sec:dis}
%
%The results of our experiment shed a lot of light on the way subjects interact with learning agents via TAMER and support prior studies (Li et al. 2014a) demonstrating the importance of bi-directional interaction and competitive elements in the training interface. %Specifically, our results show that `competition' can significantly improve agent learning especially trained by those who can train agents to perform better. 
%%When examining the gender, our results demonstrate that `facial expression'---telling subjects to use facial expressions as a separate training channel has a significantly negative effect on agent training by females subjects, and `competition' can significantly improve agent learning trained by male subjects.  Furthermore, `competition' can significantly motivate subjects older than 30 years old to train agent better, but has some negative effect on agent learning trained by subjects younger than 13 years old. 
%However, though the results are significant, some differences between the mean performances are not big. We believe that it could be because of the game score mechanism. For example, when motivated by `competition', trainers may have encouraged behaviors such as collecting mushrooms and killing a monster, which are much more difficult than collecting coins. However, the scoring metric rewards the agent equally when it kills monsters, collects coins, and no score for collecting mushrooms. The trainers may have put more emphasis on these difficult behaviors without a large increase in score, especially those subjects who can train agents to be in the second and third modes in Figure \ref{HisFinalOfflinePerformance}. 


%while a good strategy for getting a high score is to singlemindedly move to the right as fast as possible and reach the finishing line. %, even though the two are rewarded equally by the scoring metric.   Trainers may also have trained the agent to play the way they themselves play, e.g., walking back and forth to collect mushrooms or coins or smash blocks, while a good strategy for getting a high score is to singlemindedly move to the right as fast as possible and reach the finishing line.%However, a key limitation of our results is that, despite the large number of subjects, they are not statistically significant. One possible reason is that trainers, due to their prior familiarity with the domain, may have tried to teach the agent behavior that they consider good but which is not rewarded by the scoring metric from the Reinforcement Learning Competition that we employed.  For example, the scoring metric only rewards the agent when it kills monsters, collects coins, and reaches the finishing line but trainers may have also encouraged other behaviors such as collecting mushrooms.  Similarly, since killing a monster is much more difficult than collecting coins, the trainers may have put more emphasis on the former, even though the two are rewarded equally by the scoring metric.   Trainers may also have trained the agent to play the way they themselves play, e.g., walking back and forth to collect mushrooms or coins or smash blocks, while a good strategy for getting a high score is to singlemindedly move to the right as fast as possible and reach the finishing line.  A similar effect was observed by Li et al.\ \cite{li2014leveraging,li2014learning}, in which subjects training TAMER agents to play Tetris seemed to encourage clearing multiple lines at a time, though this yielded no additional rewards in the scoring metric they employed.  Teaching such complex behaviors in such a short time is difficult and, if it is not rewarded by the scoring metric, may result in the poor performance seen in the first modes in the histograms in Figure \ref{HisFinalOfflinePerformance}. Further analysis is needed to get a deeper understanding of these results, e.g., analyzing the agent's learned behavior and the contribution of each element for the performance, evaluating performance with other scoring metrics, etc.


%Interestingly, similar gender differences were also observed by other researchers \cite{brody1993understanding}, where females express a wider variety of emotions than do males, both verbally and through facial expressions. This could relate to our results about the significant effect of `facial expression' on agent performance trained by female subjects, though more investigation needs to be made into whether females made more facial expressions than males in our experimental setting.
%%In addition, more analysis of the video data is needed to
%%understand the reason for the negative effect caused by %telling subjects to use facial expressions as a separate training channel, 
%%`facial expression', e.g., to determine whether the negative effect on
%%female subjects is due to females making more pronounced facial
%%expressions. 
%However, these results point the way to further
%analysis of the correlation between key press feedback and
%facial expressions by considering gender and age differences.
%%opening the door to TAMER systems that can learn only from facial expressions. 
%%It also underscores the fact that, if we
%%want the agent to learn from facial expressions and we tell
%%trainers that they can use facial expressions to train the
%%agent, then we need to find a way to balance these negative
%%effects. Otherwise, it may be better not to let them know that
%%the agent can learn from facial expressions at all.


%\paragraph{Analysis of Facial Feedback}
\subsection{Analysis of Facial Feedback}

It is well known that facial expressions reflect inner feelings
and emotions. We assessed the informativeness of facial
expressions as a feedback signal. To this end, 3-D locations of 512
densely defined facial landmarks (see
Figure~\ref{landmarks}) were automatically detected and
tracked using the state-of-the-art method proposed by Jeni et
al.\ \cite{jeni2015dense}. Videos with a downsampled frame
rate of 20 fps were used in tracking. Data from 31 participants (5.5\% of the data analyzed) %\HH{(X\% of the data analysed)} 
were discarded due to methodological problems such as face
occlusion, talking, and chewing gum during the experiment. %\HH{state in numbers how many subjects were removed from the analysis here}. 
In
total 9,356,103 frames were tracked.  %\HH{should we say here how many hours on what types of high powered machines?}. %(110 subjects for the control condition, 101 subjects for the facial expression
%condition, 158 subjects for the competitive condition, 161
%subjects for competitive facial expression condition).
To eliminate rigid head movements, the tracked faces were
shape-normalized by removing translation, rotation and scale.
Since the normalized faces are approximately frontal with
respect to the camera, we ignored the depth ($z$) coordinates
of the normalized landmarks. \textcolor{blue}{Consequently, 1024 location
parameters were obtained per frame, i.e., $x$ and $y$ coordinates of the tracked 512 facial landmarks.} %(512 landmarks $\times$ $x$, $y$ coordinates)
%cond1: 110, 1,917,557, m=1.7432e+04, s=3.1129e+03
%cond2: 101, 1,737,242, m=1.7200e+04, s=3.4708e+03
%cond3: 158, 2,876,593, m=1.8206e+04, s=2.4188e+03
%cond4: 161, 2,824,711, m=1.7545e+04, s=2.8530e+03

\begin{figure}[htb]
\centering
\includegraphics[height=0.35\columnwidth]{markers2.pdf}
\caption{512 tracked facial landmarks.}
\label{landmarks}
\end{figure}
%\vspace{-4mm}


\begin{figure}[t!]
\begin{center}%\HH{could perhaps get away with making this table half a column and have the faces in two rows ... it feels like it's quite big or even just 4 in a row and remove (a)... do we really need (a)?}

\begin{tabular}{c c c c c}
 % \includegraphics[height=0.4\columnwidth]{markers2.pdf} &
 \includegraphics[height=0.3\columnwidth]{fc1-2.pdf}&
  \includegraphics[height=0.3\columnwidth]{fc3-4.pdf} & \\ 
  (a)& (b)&\\
  \includegraphics[height=0.3\columnwidth]{fc1-3.pdf}&
  \includegraphics[height=0.3\columnwidth]{fc2-4.pdf}   &\includegraphics[height=0.3\columnwidth]{fc_legend.pdf}  \\
   (c)& (d)\\%& (e)\\
 \end{tabular}
\end{center}
%\vspace{-4mm} 
\caption{%(a) 512 tracked facial landmarks.
Significance level ($p_c$) of differences in expressiveness
between different conditions: (a) \emph{control vs. facial
expression}, (b) \emph{competitive vs. competitive facial
expression}, (c) \emph{control vs. competitive}, and (d)
\emph{facial expression vs. competitive facial expression}.
Dark gray or black colored ($p_c<0.05$) regions display
significantly different activeness between the corresponding
conditions.} \label{markers}
%\vspace{-4mm}
\end{figure}

\subsubsection{Facial Expressiveness}

To analyze facial activity levels of different conditions, we
computed the standard deviation of the landmark positions within a 2$s$
interval around each key press feedback (1$s$ before to 1$s$
after). The 2$s$ interval is chosen because the great majority of facial expressions of felt emotions last between 0.5 second and 4 seconds \cite{ekman1984expression} and the differences of standard deviations between conditions are largest in this case. Standard deviations were averaged for each
subject in each condition. \textcolor{blue}{Then, we analyzed the significant
differences in vertical ($y$) and horizontal ($x$) facial
movements, between different conditions using a one tailed t-test.} The computed
$p$ values for $x$ and $y$ coordinates were combined as
$p_c=\sqrt{{p_x}^2+{p_y}^2}$ to represent the significance level
for each landmark position as one parameter.



Figures~\ref{markers}(a--d) visualize the significance level of
differences ($p_c$) in expressiveness between different
conditions. The visualized $p_c$ values for transition
locations between detected landmarks were computed by linear
interpolation. \textcolor{blue}{Note that to measure the single effect of ``competition'' and ``facial expression'',  we only compare differences in expressiveness in control condition with facial expression condition, control condition with competitive condition, competitive condition with competitive facial expression condition, facial expression condition with competitive facial expression condition}. When we analyze the significant differences
($p_c<0.05$) in facial activeness between different conditions,
it is seen that the mouth region is more dynamic in the \emph{facial
expression condition} compared to the \emph{control condition}
(Figure~\ref{markers}(a)). Similarly, the deviation of movements in the 
mouth, upper cheek, and forehead regions for the \emph{competitive
facial expression condition} are higher than those of the
\emph{competitive condition} (Figure~\ref{markers}(b)). These
results can be explained by the fact that subjects exaggerate
their expressions in facial expression conditions. In
competitive conditions, almost the whole surface of the face has
higher activity levels in comparison to the control conditions as
shown in Figure~\ref{markers}(c--d). These findings suggest
that competitive conditions can elevate facial expressiveness. 

\textcolor{blue}{In summary, consistent with our hypotheses, %both agent's competitive feedback and 
telling trainers to use facial expressions as additional channel to train agents could increase the trainer's facial expressiveness. Moreover, competition can elevate trainer's facial expressiveness. In addition, the largest difference was observed in Figure~\ref{markers}(c) between the control and competitive conditions, which indicates that the effect of the competitive situation on expressiveness could be larger than that effect of the facial expression condition.} %Moreover, 
%some facial expressions seems to exaggerate eyebrow only regions compared to the baseline. Could this indicate that there is more negative feedback (frowning being observed).
%ask jeff
%\HH{do we have space to discuss the fact that some facial expressions seems to exaggerate eyebrow only regions compared to the baseline. Could this indicate that there is more negative feedback (frowning being observed?)}




\subsubsection{Classification of Positive and Negative Feedback with Facial Expressions}
\label{sec:classify}


Next, we investigated the discriminative power of facial
responses for classifying positive and negative feedback. To
obtain a compact representation for facial movement parameters,
Principal Component Analysis (PCA) was used to reduce 1024
coordinate values to 20 principle components while retaining
$99.6\%$ of the variance. The mean, median, standard deviation,
maximum, minimum values of each principle component for 2$s$
intervals around each positive and negative feedback key press
(1$s$ before to 1$s$ after) were computed. These statistics can fairly describe the temporal movement characteristics of expressions within that interval \cite{cohn2009detecting,dibeklioglu2015recognition,dibekliouglu2015multimodal}. By
concatenating these features, 100-dimensional feature vectors were
obtained. Using a t-test, features that do not significantly
differ ($p>0.001$) between positive and negative feedback were
% identified, and 
removed. Random forest classifiers \cite{breiman2001random} %\SW{cite} 
using 300 trees were then trained using the resulting features.
 %(n predictors for splitting at each node).

%The random baseline is calculated based on the probabilities of 
%the positive and negative class. 

\begin{table}[t!]
%\vspace{-2mm}
%\vspace{2mm}
\begin{center}
\caption{\label{table:feedAcc} Accuracy of classifying positive
and negative feedback using facial responses.}
\resizebox{0.85\columnwidth}{!}{ \footnotesize
\renewcommand\arraystretch{1}
\begin{tabular}{clccc}
\toprule[.8pt]
                                &Condition&Positive	&Negative	&Total\\
\midrule[.8pt]
\multirow{3}{*}{\rotatebox{90}{{\parbox{1.35cm}{\footnotesize{Proposed Method}}}}} &&&&\\ [-2.5ex]
&Control                        &0.47    &0.68    &0.57	\\
&Facial Expression              &0.49    &0.69    &0.59 	\\
&Competitive                    &0.61    &0.59    &0.60  \\
&Competitive Facial Expression  &0.72    &0.59    &0.67  \\
\midrule[.8pt]
\multirow{4}{*}{\rotatebox{90}{{\parbox{1.2cm}{\footnotesize{Random Baseline}}}}} &&&&\\ [-2.5ex]
&Control                        &0.50	    &0.50	    &0.50 \\
&Facial Expression              &0.42	    &0.58	    &0.51 \\
&Competitive                    &0.52 		&0.48		&0.50 \\
&Competitive Facial Expression  &0.58 		&0.42 		&0.51 \\
\bottomrule[.8pt]
\end{tabular}
}%\vspace{.4cm}
%\caption{\label{table:feedAcc} Accuracy of classifying positive
%and negative feedback using facial responses.} %\vspace{.4cm}
\end{center}
%\vspace{-6mm}
\end{table}


Our experimental scenario learns facial patterns of the
users in the first five minutes of the game, and classifies their
positive and negative feedback during the rest of the game. A five-fold cross-validation scheme was used. While
feedback in the first five minutes from fold $i$, and all
feedback from the remaining folds were used for feature
selection and training, unseen feedback from fold $i$ were
used for experimental evaluation (around 10 minutes). There was no subject overlap between
folds. %\textcolor[rgb]{1.00,0.00,0.00}{
85,429 positive and 79,585
negative feedback instances were used in the experiment.
%} 
 \textcolor{blue}{To show the effect and accuracy of our proposed method, a random baseline is also shown for comparison. 
Class labels for random baseline are assigned by drawing a random class label according to the ratio of positive and negative class labels from the training set.}
As shown in
Table~\ref{table:feedAcc}, the use of facial expressions
significantly ($p<0.001$) outperformed the random baseline in each
condition except for classifying positive feedback in the 
\emph{control} condition. The highest accuracy was achieved for the
\emph{competitive facial expression} condition, followed by the
\emph{competitive} condition. This can be explained by the
increased facial expressivity due to the competitive setting and posed facial expressions. As
expected, the proposed method provided higher accuracies for
facial expression conditions.
% to
% \emph{control} and \emph{competitive} conditions.


%Overall, our study takes a first step towards enabling agents to learn from facial expressions by investigating correlations between facial expressions and key-press feedback.  Obviously, a critical future step would be to use classifiers trained in this way to actually train agents, thus reducing the need for explicit key-press feedback.  

%Unfortunately, such a step is well beyond the scope of this paper since it cannot be accomplished using the data gathered for our study. This is because learning via TAMER, as in any reinforcement-learning setting, is an interactive process, in which the learner's behavior affects what states are visited and thus what data is gathered.  Since all our data was gathered by agents learning only from key-press feedback, training agents using facial-expression feedback on this same data would not be meaningful: if performance declined, there would be no way to determine whether the cause was a change in the form of feedback or a discrepancy between the states visited by the recorded data and those visited by the learned policy.  
%
%Consequently, training agents using facial-expression feedback will require conducting a new study with new subjects, an important avenue for future work.


\subsection{Learning from Facial Feedback}

\textcolor{blue}{Obviously, a critical next step is to examine whether an agent can learn from the human trainer's facial expressions with the trained model in Section \ref{sec:classify} to predict human reward based on human trainer's facial expressions. }%at the time when keypress feedback was given. 
Ideally, we would run a new experiment where the trainer gives new facial expressions while watching the agent learn and use the predictor to get reward that we trained with. But that's prohibitively expensive so we need to do an evaluation with the data we collected, and the closest approximation is to get predicted human reward based on facial expressions at the time when keypress feedback was given for the complete training trajectory of each trainer. Then we use the predicted feedback instead of the keypress feedback to train the agent for the complete trajectory. %with the data we have. 

\textcolor{blue}{%Why ?????
For the predictive model training (random forest in our case), ideally, we should not incorporate the data from a new user but only use collected data from other trainers. However, we tried to train the model in this manner and found the accuracy of prediction is very low because of the large variance of facial expressiveness between subjects. 
%Since we want to do an evaluation with the collected data to get predicted feedback for the complete training trajectory of each trainer, we consider to use both the data from the user we are trying to predict and other subjects to train the model.  
Therefore, since we only want to test whether the agent can learn from facial expressions with collected data, to get predicted feedback for a complete trajectory and a higher prediction accuracy, we consider to use both the data from the user we are trying to predict and other subjects to train the model.} 
%Specifically, we trained two models for feedback prediction, with one model being trained using data in early training of one subject and data of other subjects to predict unseen feedback in later training part of the same subject and the other model being trained using data in later training of one subject and data of other subjects to predict unseen feedback in early training part of the same subject. 


\textcolor{blue}{%As the number of keypress feedback decreased in the training process shown in Figure \ref{feedback}, we chose five minutes as the division point for data of each subject to get roughly the same amount of training data for each model training. 
Specifically, as in Section 6.3.2, a five-fold cross-validation scheme was used first by dividing all subjects into five folds, i.e., %each fold data is composed of the data of subjects in the same fold. So 
there was no subject overlap between folds. Then for data in each fold, we divide it into two parts: data in the first five minutes of the game and data after the first five minutes of the game. We chose five minutes as the division point for data of each subject to get roughly the same amount of training data for each model training. 
Then we trained two predictive models to get all predictive feedback based on facial expressions for the complete training trajectory of each trainer in our data. 
The first model was trained in the same way as in Section 6.3.2.
%For model training with data in the first five minutes of the game to predict the unseen feedback during the rest of the game, we use data set in the first five minutes of the game from fold $i$ and all data from the remaining folds for feature selection and training, unseen feedback during the rest of game from fold $i$ was predicted (around 10 minutes) with the trained model. 
The second model was trained using %For model training with data after the first five minutes of the game to predict the unseen feedback in the first five minutes of the game, we use 
dataset after the first five minutes of the game from fold $i$ and all data from the remaining folds for feature selection and training, unseen feedback during the first five minutes of the game from fold $i$ was predicted with the second trained model. }%With the two predictive models, we get all predictive feedback based on facial expressions for the complete training trajectory of each trainer in our data.}

%\textcolor{blue}{Specifically, we first trained a predictive model (i.e., the trained random forest classifier, as in Section \ref{sec:classify}) with data in the first five minutes of the game to predict the unseen feedback during the rest of the game.} For predicting unseen feedback in the first five minutes of the game, we trained a second predictive model with the data during the training after the first five minutes of the game. Then we get all predictive feedback based on facial expressions for the complete training trajectory of each trainer. For each model training, the training process is the same as the process in Section \ref{sec:classify}. %, except that the model training was done with data after the first five minutes' training. 
%after the first five minutes' training, Our experimental scenario learns facial patterns of the users in the first five minutes of the game, and classifies their positive and negative feedback during the rest of the game. 
%Specifically, a five-fold cross-validation scheme was used first by dividing all subjects into five folds, i.e., each fold data is composed of the data of subjects in the same fold. So there was no subject overlap between folds. Then for data in each fold, we divide it into two parts: data in the first five minutes of the game and data after the first five minutes of the game.} 

%\begin{figure}[htb]
%%\vspace{2mm}
%%\hspace{4mm}
%\centering
%%\subfigure[Control condition and Facial Expression condition]{
%\begin{tabular}{c}
%\includegraphics[width=0.65\columnwidth]{cond1.pdf} \\
%(a) Control condition \\
%\includegraphics[width=0.65\columnwidth]{cond2.pdf} \\
%(b) Facial Expression condition \\
%\end{tabular}%}
%\caption{Offline performance of agent learning from predicted feedback with facial expressions, compared to learning from keypress feedback and random feedback for all four conditions: (a) Control condition and (b) Facial Expression condition, (c) Competitive condition and (d) Competitive Facial Expression condition on next page.}
%\end{figure} 
%
%\addtocounter{figure}{-2}
%\begin{figure}[htb]
%\centering
%\addtocounter{figure}{1} 
%%\subfigure[Competitive condition and Competitive Facial Expression condition]{
%\begin{tabular}{c}
%\includegraphics[width=0.65\columnwidth]{cond3.pdf} \\
%(c) Competitive condition \\ 
%\includegraphics[width=0.65\columnwidth]{cond4.pdf}\\
%(d) Competitive Facial Expression condition
%%\vspace{-3mm}
%\end{tabular}%}
%\caption{Offline performance of agent learning from predicted feedback with facial expressions, compared to learning from keypress feedback and random feedback for all four conditions: (a) Control condition and (b) Facial Expression condition on previous page, (c) Competitive condition and (d) Competitive Facial Expression condition.}%(a) control condition, (b) facial expression condition, (c) competitive condition and (d) competitive facial expression condition.}%: a) Control condition, b) FE condition, c) Competitive condition and d) Competitive FE condition (FE=Facial Expression). %\HH{Could it make sense to actually color the bars here according to the highlest level that was reached for a given bar? It might help in better expalaining the multi-level scoring}}
%%\vspace{-6mm}
%\label{learning_from_predicted}
%\end{figure}

%For example, for model training with data in the first five minutes of the game to predict the unseen feedback during the rest of the game, we use data set in the first five minutes of the game from fold $i$ and all data from the remaining folds for feature selection and training, unseen feedback during the rest of game from fold $i$ was predicted (around 10 minutes) with the trained model. For model training with data after the first five minutes of the game to predict the unseen feedback in the first five minutes of the game, we use data set after the first five minutes of the game from fold $i$ and all data from the remaining folds for feature selection and training, unseen feedback during the first five minutes of the game from fold $i$ was predicted with the trained model.

%While feedback in the training data set from fold $i$, and all feedback from the remaining folds were used for feature selection and training, unseen feedback from fold $i$ were used for experimental evaluation (around 10 minutes). 

%which is the same above process in Section \ref{sec:classify}.  %not use the early data to train the predictor but save it for implicit learning once we have trained the predictor on later data, or vice versa. 

%Specifically, to get predicted human reward for the complete training trajectory for each participant, we first trained the model with the same above process in Section \ref{sec:classify}, to predict the unseen feedback after the first five minutes' training. 



%In addition, because the durations of one episode for agents trained by different trainers are different and there is a pause of three seconds at the end of each episode, the number of episodes in the first five minutes is different for different trainers (there are several episodes' difference usually).  %the absolute time of one time step is a bit different for different agents,
%Therefore, the number of time steps in the first five minutes of the game is different for agents trained by different trainers, i.e., the switch time of the trained predictive model based on facial expressions in terms of number of time steps is different. However, as the difference is not large, the first five minutes of the game equal to 1000 time steps on average, i.e., we switch the trained model at about 1000 time steps for each agent. With the two predictive models, we get all predictive feedback based on facial expressions for the complete training trajectory of each trainer in our data.

Then we use the predicted feedback to train agents with collected data and test the offline performance of the learning policy for 20 games each. Specifically, the agent continually learns from predicted feedback, and we record the learning policy per 200 time steps and test the policy for 20 games. Then the agent continues learning from predicted feedback for another 200 time steps with policy recorded and tested for 20 games each. This process repeats up to 2800 time steps. %i.e., the agent continually learns from predicted feedback, and every 200 time steps the learned policy is recorded.

We also tested the performance of agent's learning policy from key press feedback in our recorded data and agent's learning performance from random feedback. The process of learning from predicted random feedback is the same as the one of learning from predicted feedback based on facial expressions except the feedback is generated randomly (with probability of 0.5 for positive and negative feedback each). The whole process was repeated for three trials for both generating and learning from predicted feedback and random feedback. The performance for each condition is averaged over the three trials.  %\sw{I think you need to much more clearly motivate why you are doing the evaluation in this way.  You need to say that, ideally, you'd run a new experiment where the trainer gave new facial expressions while watching the agent learn and you used your predictor to get reward that you trained with.  But that's prohibitively expensive so you need to do an evaluation with the data you have, and the closest approximation is to not use the early data to train the predictor but save it for implicit learning once you've trained the predictor on later data.}

%\newpage

\begin{figure}[htb]
%\vspace{2mm}
%\hspace{4mm}
\centering
\begin{tabular}{c c}
\includegraphics[width=0.45\columnwidth]{cond1.pdf} &
\includegraphics[width=0.45\columnwidth]{cond2.pdf} \\
(a) Control condition & (b) Facial Expression condition \\
\includegraphics[width=0.45\columnwidth]{cond3.pdf} &
\includegraphics[width=0.45\columnwidth]{cond4.pdf}\\
(c) Competitive condition & (d) Competitive Facial Expression condition
%\vspace{-3mm}
\end{tabular}
\caption{Offline performance of agent learning from predicted feedback with facial expressions, compared to learning from keypress feedback and random feedback for all four conditions. }%(a) control condition, (b) facial expression condition, (c) competitive condition and (d) competitive facial expression condition.}%: a) Control condition, b) FE condition, c) Competitive condition and d) Competitive FE condition (FE=Facial Expression). %\HH{Could it make sense to actually color the bars here according to the highlest level that was reached for a given bar? It might help in better expalaining the multi-level scoring}}
%\vspace{-6mm}
\label{learning_from_predicted}
\end{figure}


We compare the agent's learning performance from predicted feedback to random feedback and keypress feedback in all four conditions, as shown in Figure \ref{learning_from_predicted}. From Figure \ref{learning_from_predicted} we can see that for agent's learning from all three kinds of feedback, the agent's learning performance quickly goes up at the beginning of the training process and goes down afterwards. From our observation in the experiment, trainers started to train complex behaviors for Mario after training for a while, which caused the decrease of performance in the middle of training, even with keypress feedback. At the beginning, it is easy to train the agent to run to the right to complete the level, which will get the most score (100). After training for some time, trainers started to train complex behaviors, e.g., picking up mushroom, which gets no point at all. In this case, they need to train the agent to unlearn the right running behavior, which cause Mario to stop and go left. In addition, the Mario agent will get -0.01 score for one time step longer in the game. %With keypress feedback, it can recover from the decrease of performance, but with predicted feedback and low accuracy, it would be much more difficult.

%Another factor is that people started to train a complex behaviors for Mario after training for a while, which caused the decrease of performance in the middle of training, even with keypress feedback. At the beginning, it is easy to train the agent to run to the right to complete the level, which will get the most score (100). After some training, trainers started to train complex behaviors, e.g., picking up mushroom, which gets no point at all. In this case, they need to train the agent to unlearn the right running behavior, which cause Mario to stop and go left. With keypress feedback, it can recover from the decrease of performance, but with predicted feedback and low accuracy, it would be much more difficult.

%The unlearning curve could be caused by the low predicted accuracy. 

%For the learning curve goes up at the beginning with random feedback, I think since we only predict the labels when there was actually key press feedback given, at the beginning, it is possible to make right predictions. When Mario is initialized to be standing still and moving to left, it is still possible to for Mario learn the right running behavior even some feedback is wrong. For example, when there were 3 negative feedback and 1 positive feedback, the whole effect of feedback is still negative. However, as the training progresses, the effect of randomness will accumulate and not benefit the agent?s learning finally.

%For learning from keypress feedback, . 
%For learning from keypress feedback, the agent's learning reaches the bottom at the middle of the training process and can still recover from the falling of performance, while agent's learning from both predicted feedback and random feedback are getting worse until the end of training. 

With keypress feedback, the agent can relearn the right running behavior and recover from the decrease of performance, but with predicted feedback of low accuracy and random feedback, it would be much more difficult. \textcolor{blue}{However, from Figure \ref{learning_from_predicted} we can see that agent learning from facial expressions performs better than from random feedback, though still worse than learning from %explicit 
keypress feedback (probably because of the low predicted accuracy). Even though the learning from facial expressions is modest, the fact that the agent is able to learn at all given only facial expressions %implicit feedback 
is an exciting success and opens up a lot of new potentials for learning from human reward that should be explored. We believe in future work, an improved model with higher prediction accuracy would allow the agent to learn better from facial expressions, though investigations with real experiments are needed to judge its general applicability.}


%both competition and telling to use facial expressions can elevate the human trainer's facial expressiveness and result in higher accuracy of predicting positive and negative feedback using facial responses, 
%taking the first step for exploring methods of facilitating agents to learn from facial expressions in future work.

%When considering gender, our results suggest that telling female subjects to use facial expressions as a separate training channel has a negative effect on training agents but not for male subjects. However, the agent's competitive feedback can improve the agent's learning only when female subjects were told to use facial expressions as a separate training channel and when male subjects were not told to. 

%When considering age group, our results suggest that the agent's competitive feedback and telling subjects to use facial expressions as a separate training channel have different effects on different age groups. Specifically, telling subjects to use facial expression as a separate training channel has little or even no effect on the agent's learning for subjects less 30 years old, but has a negative effect on the agent's learning for subjects more than 30 years old. In addition, the agent's competitive feedback can improve the agent's learning when trained by subjects more than 30 years old, has no clear effect on subjects less than 13 years old, and improves the agent's learning for subjects between 13 and 30 years old only when they are told to use facial expressions as a separate training channel. %Finally, these findings provide a deep insight into how to use facial expression as reward signal to train the agent.

%From these analysis we can see that, generally speaking, knowing facial expression as training signal has negative effect on agent training, competitiveness can increase agent performance and competitiveness can compensate this negative effect. In term of different age groups, this phenomenon can be found especially on subjects older than 31 years old. But for subjects younger than 30 years old it does not work well or it may work person-specificly or gender-specific.
%
%In term of gender difference, generally speaking, knowing facial expression as training signal has negative effect on agent training happens on female subjects but unclear for male subjects. However, after looking into the gender difference within different age groups, we found that this negative effect happens on female subjects younger than 30 years old but on male subjects older than 30 years old.
%
%On average, competitiveness only works on female subjects when they were told to use facial expression as training signal, but unclear for male subjects. After looking into the gender difference within different age groups, for female subjects, competitiveness works whenever they were told to use facial expression as training signal for those older than 31 years old, but for those younger than 12 years old, it only works when they were told to use facial expression as training signal. This phenomenon remains unclear for female subjects between 13 and 30 years old.
%
%For male subjects, for for those younger than 12 years old, competitiveness works they were not told to use facial expression as training signal, but for those older than 13 years old, it only works when they were told to use facial expression as training signal.

%Future work will focus on further investigating the role of facial expressions in agent learning from human reward and its effects as a function of age and gender. This will enable us to explore the potential of allowing the agent to learn from it. 
%%\HH{I removed this part - we don't want to give away too many of our ideas!} First, we aim to investigate whether there is a correlation between facial expressions and explicit human reward. Second, we aim to use machine learning to train functions that predict explicit reward given implicit reward, so that implicit reward can be directly used within TAMER even when explicit feedback is not available.  
%Finally, we aim to find a way to balance the negative effects caused by telling trainers to use facial expressions as a reward signal, so as to maximize its potential as a learning signal.

\section{Discussion and Open Questions}
\label{sec:doq}

\GL{The reference to and ideas for potential use cases which could benefit from the drawn insights would be desirable (e.g. from the social robotics or virtual agents domain, at least for future work and discussion).}

%\GL{Please elaborate these aspects and whether you expect your findings to be applicable outside the TAMER framework or the Infinite Mario domain, too.}

\textcolor{blue}{Our work contributes to the design of human-agent systems that facilitate the agent to learn more efficiently and be easier to teach. To our knowledge, we are the first to highlight clearly that the relationship between the reward signal and the nature of facial expressions. %provide the analysis of how manipulating the information the agent shares with the trainer can affect the trainer's behavior. 
We demonstrate that an understanding of how to design the interaction between the agent and the trainer allows for the design of the algorithms that support how people can teach effectively and be actively engaged in the training process at the same time. This is useful for personalizing interaction with a socially assistive robotics, e.g., an educational assistive robot tutor \cite{gordon2016affective} trying to teach children a second language. Facial expression interaction can also be used for children with autism to improve these children's social interaction abilities \cite{pour2018human}. In such cases, facial expression can be extracted as feedback for personalizing the interaction process for users with different abilities. Human rewards given without the intention to teach or otherwise affect behavior?possibly derived from smiles, attention, tone of voice, or other social cues are more abundantly broadcast and can be ob- served without adding any cognitive load to the human \cite{knox2012learning}.} 

\textcolor{blue}{More recently, research has also focused on developing robots that can detect common human communication cues for more natural interactions. Social HRI is a subset of HRI that encompasses robots which interact using natural human communication modalities, including speech, body language and facial expressions. This allows humans to interact with robots without any extensive prior training, permitting desired tasks to be completed more quickly and requiring less work to be performed by the user \cite{mccoll2016survey}. 
The systems and techniques discussed above focus on the recognition of one single input mode in order to determine human affect. The use of multimodal inputs over a single input provides two main advan- tages: when one modality is not available due to disturbances such as occlusion or noise, a multimodal recognition system can estimate affective state using the remaining modalities, and when multiple modal- ities are available, the complementarity and diversity of information can provide increased robustness and performance}

\textcolor{blue}{%Our experiments highlight clearly that the relationship between the reward signal and the nature of facial expressions. 
However, how human trainers choose to use them still needs to be explored further. In addition, our further results with recorded data indicate that an agent can learn from facial expressions via interpreting as reward signals, though the learning is modest. The results in this article suggest that a more natural interaction design for agent training is important. However, such rewards are untargeted; someone might be smiling for any number of reasons that have nothing to do with the agent. Conse- quently, interpretation and attribution of these social cues will be especially challenging. }%a performance metric for the task defined from the human teacher's perspective will be useful for the training process. more natural interaction.}

\textcolor{blue}{Consequently, understanding the influence of agent's social competitive feedback %metric-sharing 
on human behavior could be a powerful guiding principle in the design of interactive interfaces for training agents, though more investigation is needed to judge its general applicability. Moreover, 
However, before agents or social robots begin to learn feedback signals from facial expressions in fairly unconstrained settings, we highlight a few future challenges: }%Our work contributes to the design of human-agent systems that facilitates the agent to learn more efficiently and be easier to teach. To our knowledge, we are the first to provide the analysis of how manipulating the information the agent shares with the trainer can affect the trainer?s behavior. We demonstrate that an understanding of how to design the interaction between the agent and the trainer allows for the design of the algorithms that support how people can teach effectively and be actively engaged in the process at the same time. 
%Our results also point out the training interchangeability between training trajectories regarding how much training data can be reused. Our approach can also apply to other interactive learning algorithms, e.g., learning from demonstration.}

%\textcolor{blue}{Finally, we believe that our approach could transfer to other domains and methods for agent learning from a human, since TAMER succeeds in many domains including Tetris, Mountain Car, Cart Pole, Keepaway Soccer, Interactive Robot Navigation etc. \cite{knox2012learning,knox2013training}.}
%Training agents using facial-expression feedback will require conducting a new study with new subjects. However, our results with recorded data indicate that an agent can learn from facial expressions via interpreting as reward signals, though the learning is modest.
%Unfortunately, such a step is well beyond the scope of this paper since it cannot be accomplished using the data gathered for our study. This is because learning via TAMER, as in any reinforcement-learning setting, is an interactive process, in which the learner's behavior affects what states are visited and thus what data is gathered.  %Since all our data was gathered by agents learning only from key-press feedback, training agents using facial-expression feedback on this same data would not be meaningful: if performance declined, there would be no way to determine whether the cause was a change in the form of feedback or a discrepancy between the states visited by the recorded data and those visited by the learned policy.  
%Consequently, training agents using facial-expression feedback will require conducting a new study with new subjects, an important avenue for future work. 
%However, before agents begin to learn feedback signals from facial expressions in fairly unconstrained settings, we highlight a few future challenges:

\subsection{Efficiency of using face and keypress input}
Our experiments highlight significant differences between the performance of agents when participants were told facial expressions would be used and when they were not. Importantly, despite differences in the quality of key-press feedback in the facial expression conditions, the number of feedback instances was not significantly different. This suggests a negative effect related to the distraction %cognitive load 
of being told to make facial expressions. To measure the effects of distraction %cognitive load 
fully in this setting remains an open challenge since we do not know fully what the facial expressions mean when they are more spontaneous and whether they are related to the agent's behavior in the task or not. This is evidenced by the results in Table \ref{table:feedAcc}, where the reward signal of the facial expressions in the control condition were the most difficult to predict. 

\subsection{Feedback quality in relation to temporal factor}
As shown in Figure \ref{feedback}, the amount of feedback reduces over time in general. Does keypress feedback quality reduce as a result? Does facial feedback correspond to feedback better quality as time increases? Further investigation is necessary to understand whether the nature of the feedback differs in terms of reaction time, length of expression (the time it takes to go from neutral to the expression and back to neutral), intensity of the facial expression, amongst others.


\subsection{Linking facial behavior to positive and negative reward}
We see that predicting facial expressions in more competitive scenarios is generally easier, particularly if the subjects are told that posed expressions correspond to positive and negative reward. However, we do not know how spontaneous as opposed to posed facial expressions might contribute to agent learning. It could also be that these require the learning of different models, even within the same condition. Moreover, in our work, the prediction based on facial expressions was done at the time of giving key press feedback, we also need to understand whether facial expression feedback might also be useful for agent learning even when no key press feedback is given. 

\vspace{2mm}

\section{Conclusion}% \& Future Work} 
\label{sec:con}
%\HH{I have not edited this. I'd like to wait till the Results section is a bit more finalised.}
This article investigated the effect of an agent's competitive feedback and the potential for agents to learn from human trainers' facial expressions.
%telling trainers to use facial expressions as a separate training channel on agent's learning. %in different genders and ages. 
To this end, we conducted the first large-scale study with usable data from 498 participants (children and adults) by implementing TAMER in the Infinite Mario domain. Our results show for the first time that a TAMER agent can successfully learn to play Infinite Mario, generalizing prior work in other game environments. In addition, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface. %and shows the negative effect of telling female trainers to use facial expressions for training. %Specifically, `competition' can significantly improve agent learning especially trained by subjects who trained agents best %, male subjects 
%and subjects older than 30 years old. %. When considering gender, `competition' can significantly improve agent learning trained by male subjects. When considering age, `competition' can significantly motivate subjects older than 30 years old to train agent better. 
Furthermore, %`facial expression'---
%telling to use facial expressions as a separate training channel has a significantly negative effect on agent training by female subjects, %especially those who are younger than 13 years old and train agents insufficiently well, 
our analysis shows that telling trainers to use facial expressions makes them inclined to exaggerate their expressions, resulting in higher accuracy for predicting positive and negative feedback using facial expressions. Competitive conditions also elevated facial expressiveness and further increased predicted accuracy. This has significant consequences for the design of agent learning systems that wish to take into account a trainer's spontaneous facial expressions as a reward signal. \textcolor{blue}{Moreover, our results indicate that an agent can learn from facial expressions via interpreting as reward signals, though the learning is modest.} Further investigation into the nature of spontaneous and posed facial expressions is needed, in particular in terms of their relation to feedback quality and quantity.
\textcolor{blue}{Finally, we believe that our approach could transfer to other domains and apply to other interactive learning algorithms, e.g., learning from demonstration, %methods for agent learning from a human, 
since TAMER succeeds in many domains including Tetris, Mountain Car, Cart Pole, Keepaway Soccer, Interactive Robot Navigation etc. \cite{knox2012learning,knox2013training}.} %Our approach can also apply to other interactive learning algorithms, e.g., learning from demonstration.}

% For one-column wide figures use
%\begin{figure}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  %\includegraphics{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%
% For two-column wide figures use
%\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  %\includegraphics[width=0.75\textwidth]{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:2}       % Give a unique label
%\end{figure*}
%
% For tables use
%\begin{table}
% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliographystyle{abbrv}
\bibliography{jaamas17}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
%\end{thebibliography}

%\textbf{Guangliang Li} 
%\paragraph{} 
%\textbf{Guangliang Li} received a Bachelor and M.Sc. degree from School of Control Science and Engineering in Shandong University in 2008 and 2011 respectively. In 2016, he received a Ph.D. degree in Computer Science from University of Amsterdam, The Netherlands. He was a visiting researcher in Delft University of Technology, The Netherlands, and a research intern in Honda Research Institute Japan, Co., Ltd. Japan. He is currently a lecturer in Ocean University of China. His research interests include reinforcement learning, human agent/robot interaction and robotics. He is currently a member of IEEE.
%
%\paragraph{} 
%%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./figures/authors/hdibeklioglu.pdf}}]
%\textbf{Hamdi Dibeklio{\u g}lu} (S'08--M'15) received the the M.Sc.
%degree from Bo{\u g}azi{\c c}i University, Istanbul, Turkey, in
%2008, and the Ph.D. degree from the University of Amsterdam,
%Amsterdam, The Netherlands, in 2014. He is currently a
%Post-doctoral Researcher in the Pattern Recognition \&
%Bioinformatics Group at Delft University of Technology, Delft,
%The Netherlands. Earlier, he was a Visiting Researcher at
%Carnegie Mellon University, University of Pittsburgh, and
%Massachusetts Institute of Technology. His research interests
%include Affective Computing, Intelligent Human-Computer
%Interaction, Pattern Recognition, and Computer Vision. Dr. Dibeklio{\u g}lu was a Co-chair for the Netherlands
%Conference on Computer Vision 2015, and a Local Arrangements
%Co-chair for the European Conference on Computer Vision 2016.
%He served on the Local Organization Committee of the eNTERFACE
%Workshop on Multimodal Interfaces, in 2007 and 2010.
%\end{IEEEbiography}


\end{document}
% end of file template.tex

